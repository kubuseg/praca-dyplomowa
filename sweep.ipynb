{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g1PUxOu1Jpbw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.utils.data as data\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn as nn\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkubuseg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BTBlNr2GofNi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA RTX A3000 12GB Laptop GPU'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.set_device(0)\n",
        "device = torch.device(\"cuda\")\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_input_dataset(dataset_path):\n",
        "    return np.load(dataset_path, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X8IPYpD9ztJJ"
      },
      "outputs": [],
      "source": [
        "def interpolate_missing_joints(movie):\n",
        "    \"\"\"\n",
        "    Interpolate joint vals based on last valid value,\n",
        "    number of invalid frames and current valid frame\n",
        "\n",
        "    :param movie: has shape [Frame, Person, Axis, Joint]\n",
        "    \"\"\"\n",
        "    last_val_joint, last_val_frame = (\n",
        "        np.zeros_like(movie[0]),\n",
        "        np.zeros_like(movie[0]),\n",
        "    )\n",
        "    for idx, frame in enumerate(movie):\n",
        "        # Array that counts frames since last valid frame\n",
        "        val_frame_diff = idx - last_val_frame\n",
        "        interp_cond = (frame != 0) & (\n",
        "            val_frame_diff > 1) & (last_val_joint != 0)\n",
        "        # Iterate over (Person, Axis, Joint) tuples for interpolation\n",
        "        for index in np.transpose(interp_cond.nonzero()):\n",
        "            index = tuple(index)\n",
        "            # index - 3 item tuple (Person, Axis, Joint)\n",
        "            last_frame, num_frames = (\n",
        "                int(last_val_frame[index]),\n",
        "                int(val_frame_diff[index]),\n",
        "            )\n",
        "            delta = (frame[index] - last_val_joint[index]) / \\\n",
        "                val_frame_diff[index]\n",
        "            for fr_num in range(1, num_frames):\n",
        "                movie[(last_frame + fr_num,) + index] = (\n",
        "                    last_val_joint[index] + delta * fr_num\n",
        "                )\n",
        "        last_val_joint[frame != 0] = frame[frame != 0]\n",
        "        last_val_frame[frame != 0] = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kxhcbM56zxSE"
      },
      "outputs": [],
      "source": [
        "def fill_border_joints(movie):\n",
        "    \"\"\"\n",
        "    Fill first n invalid joint values frames with first valid\n",
        "\n",
        "    :param movie: has shape [Person, Axis, Frame, Joint]\n",
        "\n",
        "    \"\"\"\n",
        "    is_joint_valid = movie[0] != 0\n",
        "    for idx, frame in enumerate(movie[1:], start=1):\n",
        "        if is_joint_valid.all():\n",
        "            break\n",
        "        fill_cond = (frame != 0) & (is_joint_valid == False)\n",
        "        for index in np.transpose(fill_cond.nonzero()):\n",
        "            index = tuple(index)\n",
        "            for fr_num in range(0, idx):\n",
        "                movie[(fr_num,) + index] = frame[index]\n",
        "        is_joint_valid[frame != 0] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_Kw7XXmJz0GP"
      },
      "outputs": [],
      "source": [
        "def handle_missing_joints(movie):\n",
        "    if (movie == 0).sum() == 0:\n",
        "        return\n",
        "    # Movie shape: (Frame[x], Person[2], Axis[3], Joint[25])\n",
        "    interpolate_missing_joints(movie=movie)\n",
        "    # Backward fill\n",
        "    fill_border_joints(movie=movie)\n",
        "    # Forward fill\n",
        "    fill_border_joints(movie=movie[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aYstEYALAGa0"
      },
      "outputs": [],
      "source": [
        "def even_spaced_sampling(movie, out_frame_num):\n",
        "    num_frames = movie.shape[2]\n",
        "    if num_frames <= out_frame_num:\n",
        "        return movie\n",
        "    indexes = np.linspace(\n",
        "        start=0, stop=num_frames, num=out_frame_num, endpoint=False, dtype=np.int16\n",
        "    )\n",
        "    return movie[:, :, indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uniform_sampling(movie, out_frame_num):\n",
        "    movie_out = np.zeros(\n",
        "        (movie.shape[0], movie.shape[1], out_frame_num, movie.shape[-1])\n",
        "    )\n",
        "    movie_frame_num = movie.shape[2]\n",
        "    if movie_frame_num <= out_frame_num:\n",
        "        movie_out[..., :movie_frame_num, :] = movie\n",
        "        return movie_out\n",
        "    for frame_idx, frame_group in enumerate(\n",
        "        np.array_split(movie, out_frame_num, axis=2)\n",
        "    ):\n",
        "        frames = frame_group.shape[2]\n",
        "        frame_taken = np.random.randint(0, frames)\n",
        "        movie_out[..., frame_idx, :] = frame_group[..., frame_taken, :]\n",
        "    return movie_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(movie, origin_joints, coordinate_dim):\n",
        "    # Change origin to joint val from first val frame\n",
        "    origin = movie[..., 0, origin_joints].mean(axis=2)\n",
        "    for frame_idx in range(1, movie.shape[2]):\n",
        "        if (movie[..., frame_idx, origin_joints] != 0).all():\n",
        "            origin = movie[..., frame_idx, origin_joints].mean(axis=2)\n",
        "            break\n",
        "    movie = (movie.reshape((-1, movie.shape[0], coordinate_dim)) - origin).reshape(\n",
        "        movie.shape\n",
        "    )\n",
        "\n",
        "    for axis_idx in range(movie.shape[1]):\n",
        "        joint_keypoints = movie[:, axis_idx, :, :]\n",
        "        movie[:, axis_idx, :, :] = (\n",
        "            joint_keypoints - joint_keypoints.mean()\n",
        "        ) / joint_keypoints.std()\n",
        "    return movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(\n",
        "    dataset, out_frame_num, coordinate_dim, joint_number, sampling, origin_joints\n",
        "):\n",
        "    dataset_len = len(dataset[\"annotations\"])\n",
        "    labels, names = [], []\n",
        "    skelets = torch.zeros(\n",
        "        (dataset_len, 2, coordinate_dim, out_frame_num, joint_number),\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "    for idx, anot in enumerate(tqdm(dataset[\"annotations\"])):\n",
        "        labels.append(anot[\"label\"])\n",
        "        names.append(anot[\"frame_dir\"])\n",
        "        movie = anot[\"keypoint\"].astype(np.float32)\n",
        "\n",
        "        movie = movie.transpose((0, 3, 1, 2))\n",
        "        # Keypoint shape - (Person[2], Axis[3], Frame[x], Joint[25])\n",
        "        handle_missing_joints(movie=movie.transpose((2, 0, 1, 3)))\n",
        "        movie = (\n",
        "            even_spaced_sampling(movie, out_frame_num)\n",
        "            if sampling == \"even_spaced\"\n",
        "            else uniform_sampling(movie, out_frame_num)\n",
        "        )\n",
        "        movie = normalize(movie, origin_joints, coordinate_dim)\n",
        "        skelets[idx, : movie.shape[0], :, : movie.shape[2]] = torch.from_numpy(movie)\n",
        "    labels = np.array(labels)\n",
        "    names = np.array(names)\n",
        "    return labels, names, skelets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "weKSwu4Dz_vW"
      },
      "outputs": [],
      "source": [
        "def get_motion(raw_data):\n",
        "    zero_frame = torch.zeros(\n",
        "        (\n",
        "            raw_data.shape[0],\n",
        "            raw_data.shape[1],\n",
        "            raw_data.shape[2],\n",
        "            1,\n",
        "            raw_data.shape[-1],\n",
        "        )\n",
        "    )\n",
        "\n",
        "    motion = torch.diff(raw_data, axis=-2, prepend=zero_frame)\n",
        "    return motion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UKgJvIriofNq"
      },
      "outputs": [],
      "source": [
        "def get_train_val_dataset(names, labels, dataset, raw_data, motion_data):\n",
        "    is_train = np.isin(names, np.array(dataset[\"split\"][EVAL_TYPE + \"_train\"]))\n",
        "\n",
        "    is_val = np.isin(names, np.array(dataset[\"split\"][EVAL_TYPE + \"_val\"]))\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        raw_data[is_train],\n",
        "        motion_data[is_train],\n",
        "        torch.from_numpy(labels[is_train]).type(torch.LongTensor),\n",
        "    )\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(\n",
        "        raw_data[is_val],\n",
        "        motion_data[is_val],\n",
        "        torch.from_numpy(labels[is_val]).type(torch.LongTensor),\n",
        "    )\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JmSSli7aofNr"
      },
      "outputs": [],
      "source": [
        "def get_train_val_loader(batch_size, train_dataset, val_dataset):\n",
        "    g = torch.Generator()\n",
        "\n",
        "    g.manual_seed(0)\n",
        "\n",
        "    train_loader = data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        generator=g,\n",
        "    )\n",
        "\n",
        "    val_loader = data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        generator=g,\n",
        "    )\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel=3,\n",
        "        padding=1,\n",
        "        groups=1,\n",
        "        max_pool=False,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, kernel, padding=padding, groups=groups\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(dropout),\n",
        "        )\n",
        "        if max_pool:\n",
        "            self.conv += nn.Sequential(\n",
        "                nn.MaxPool2d(2),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NetConv(nn.Module):\n",
        "    def __init__(self, coordinate_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            Block(\n",
        "                coordinate_dim,\n",
        "                36,\n",
        "                kernel=(coordinate_dim, 1),\n",
        "                padding=0,\n",
        "                groups=coordinate_dim,\n",
        "                dropout=dropout,\n",
        "            ),\n",
        "            Block(36, 36, kernel=(coordinate_dim, 1), padding=0, dropout=dropout),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            Block(36, 64, kernel=3, padding=1, max_pool=True, dropout=dropout),\n",
        "            Block(64, 128, kernel=3, padding=1, max_pool=True, dropout=dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.conv1(x[:, 0])\n",
        "        x1 = self.conv1(x[:, 1])\n",
        "        x0 = self.conv2(x0)\n",
        "        x1 = self.conv2(x1)\n",
        "        return x0, x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NetConv2(nn.Module):\n",
        "    def __init__(self, dropout=0.1, input_channels=256):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            Block(\n",
        "                input_channels, 256, kernel=3, padding=1, max_pool=True, dropout=dropout\n",
        "            ),\n",
        "            Block(256, 512, kernel=3, padding=1, max_pool=True, dropout=dropout),\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, net_raw_1, net_raw_2, net_motion_1=None, net_motion_2=None):\n",
        "        if torch.is_tensor(net_motion_1):\n",
        "            x0 = torch.cat((net_raw_1, net_motion_1), dim=1)\n",
        "            x1 = torch.cat((net_raw_2, net_motion_2), dim=1)\n",
        "        else:\n",
        "            x0 = net_raw_1\n",
        "            x1 = net_raw_2\n",
        "        x0 = self.conv(x0)\n",
        "        x1 = self.conv(x1)\n",
        "        x = torch.maximum(self.flatten(x0), self.flatten(x1))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NetFF(nn.Module):\n",
        "    def __init__(self, class_number, input_features=512, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            # Linear 1\n",
        "            nn.Linear(input_features, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            # Linear 2\n",
        "            nn.Linear(256, class_number),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ff(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fileName(accuracy, net_name, skeleton_type, folder):\n",
        "    return (\n",
        "        folder\n",
        "        + net_name\n",
        "        + \"_\"\n",
        "        + skeleton_type\n",
        "        + \"_\"\n",
        "        + f\"{accuracy:.1f}\".replace(\".\", \"\")\n",
        "        + \".pt\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "\n",
        "def saveModels(accuracy, net_dict, skeleton_type, folder_str):\n",
        "    Path(folder_str).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for net_name, net in net_dict.items():\n",
        "        torch.save(\n",
        "            net.state_dict(), fileName(accuracy, net_name, skeleton_type, folder_str)\n",
        "        )\n",
        "\n",
        "\n",
        "def deleteModels(accuracy, net_dict, skeleton_type, folder_str):\n",
        "    for net_name in net_dict.keys():\n",
        "        file_name = fileName(accuracy, net_name, skeleton_type, folder_str)\n",
        "        if os.path.isfile(file_name):\n",
        "            os.remove(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 120\n",
        "DATASET_NAME = \"ntu60\"  # (ntu60, ntu120)\n",
        "EVAL_TYPE = \"xsub\"  # (xsub, xview, xset)\n",
        "SCHEDULER = \"ReduceLROnPlateau\"  # (ReduceLROnPlateau, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "CLASS_NUMBER = 60 if DATASET_NAME == \"ntu60\" else 120\n",
        "FOLDER_STR = \"models/\" + DATASET_NAME + \"/\" + EVAL_TYPE + \"/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: cvou4ffa\n",
            "Sweep URL: https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa\n"
          ]
        }
      ],
      "source": [
        "sweep_config = {\"method\": \"grid\"}\n",
        "metric = {\"name\": \"val/val_max_accuracy\", \"goal\": \"maximize\"}\n",
        "sweep_config[\"metric\"] = metric\n",
        "parameters_dict = {\n",
        "    \"optimizer\": {\"value\": \"adam\"},\n",
        "    \"eval_Type\": {\"value\": EVAL_TYPE},\n",
        "    \"dataset\": {\"value\": DATASET_NAME},\n",
        "    \"epochs\": {\"value\": EPOCHS},\n",
        "    \"scheduler\": {\"value\": SCHEDULER},\n",
        "    \"skeleton_type\": {\"value\": \"2D\"},\n",
        "    \"batch_size\": {\"value\": 128},\n",
        "    \"sampling\": {\"value\": \"even_spaced\"},\n",
        "    \"out_frame_num\": {\"value\": 32},\n",
        "    \"dropout_conv\": {\"value\": 0.1},\n",
        "    \"dropout_ff\": {\"value\": 0.5},\n",
        "    \"learning_rate\": {\"values\": [1e-3, 5e-4, 2.5e-4, 1e-4, 5e-5]},\n",
        "    \"model\": {\"value\": \"J-CNN\"},\n",
        "}\n",
        "sweep_config[\"parameters\"] = parameters_dict\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"InÅ¼ynierka\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_dataset(\n",
        "    dataset_path,\n",
        "    out_frame_num,\n",
        "    coordinate_dim,\n",
        "    joint_number,\n",
        "    sampling,\n",
        "    origin_joints,\n",
        "    batch_size,\n",
        "):\n",
        "    random.seed(0)  # python random generator\n",
        "    np.random.seed(0)  # numpy random generator\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "    dataset = get_input_dataset(dataset_path)\n",
        "    labels, names, raw_data = preprocess_data(\n",
        "        dataset, out_frame_num, coordinate_dim, joint_number, sampling, origin_joints\n",
        "    )\n",
        "    motion_data = get_motion(raw_data)\n",
        "    train_dataset, val_dataset = get_train_val_dataset(\n",
        "        names, labels, dataset, raw_data, motion_data\n",
        "    )\n",
        "    train_loader, val_loader = get_train_val_loader(\n",
        "        batch_size, train_dataset, val_dataset\n",
        "    )\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_network(\n",
        "    coordinate_dim,\n",
        "    class_number,\n",
        "    input_features=512,\n",
        "    input_channels=256,\n",
        "    dropout_conv=0.1,\n",
        "    dropout_ff=0.5,\n",
        "):\n",
        "    net_raw = NetConv(coordinate_dim, dropout_conv).to(device)\n",
        "    net_motion = NetConv(coordinate_dim, dropout_conv).to(device)\n",
        "    net_conv_2 = NetConv2(dropout=dropout_conv, input_channels=input_channels).to(\n",
        "        device\n",
        "    )\n",
        "    net_ff = NetFF(class_number, input_features, dropout_ff).to(device)\n",
        "    return net_raw, net_motion, net_conv_2, net_ff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def build_crit_optim_sched(networks, learning_rate):\n",
        "    net_raw, net_motion, net_conv_2, net_ff = networks\n",
        "    params = (\n",
        "        sum(p.numel() for p in net_raw.parameters() if p.requires_grad)\n",
        "        + sum(p.numel() for p in net_motion.parameters() if p.requires_grad)\n",
        "        + sum(p.numel() for p in net_ff.parameters() if p.requires_grad)\n",
        "        + sum(p.numel() for p in net_conv_2.parameters() if p.requires_grad)\n",
        "    )\n",
        "    print(f\"Model parameters: {params:,}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    params = (\n",
        "        list(net_raw.parameters())\n",
        "        + list(net_motion.parameters())\n",
        "        + list(net_conv_2.parameters())\n",
        "        + list(net_ff.parameters())\n",
        "    )\n",
        "    optimizer = optim.Adam(params, lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", factor=0.5, patience=10, min_lr=1e-5, verbose=True\n",
        "    )\n",
        "    return criterion, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_output(models, inputs, model_name):\n",
        "    net_raw, net_motion, net_conv_2, net_ff = models\n",
        "    raw, motion = inputs\n",
        "    if model_name == \"2S-CNN\":\n",
        "        raw_out_1, raw_out_2 = net_raw(raw)\n",
        "        motion_out_1, motion_out_2 = net_motion(motion)\n",
        "        out_conv = net_conv_2(raw_out_1, raw_out_2, motion_out_1, motion_out_2)\n",
        "    elif model_name == \"1S-CNN\":\n",
        "        raw_out_1, raw_out_2 = net_raw(torch.cat((raw, motion), dim=-1))\n",
        "        out_conv = net_conv_2(raw_out_1, raw_out_2)\n",
        "    elif model_name == \"J-CNN\":\n",
        "        raw_out_1, raw_out_2 = net_raw(raw)\n",
        "        out_conv = net_conv_2(raw_out_1, raw_out_2)\n",
        "    outputs = net_ff(out_conv)\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getAccuarcy(loader, networks, model):\n",
        "    net_raw, net_motion, net_conv_2, net_ff = networks\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    net_raw.eval()\n",
        "\n",
        "    net_motion.eval()\n",
        "    net_conv_2.eval()\n",
        "\n",
        "    net_ff.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for raw, motion, labels in loader:\n",
        "            raw, motion, labels = raw.to(device), motion.to(device), labels.to(device)\n",
        "\n",
        "            outputs = get_model_output(\n",
        "                models=(net_raw, net_motion, net_conv_2, net_ff),\n",
        "                inputs=(raw, motion),\n",
        "                model_name=model\n",
        "            )\n",
        "\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    networks, train_loader, val_loader, optimizer, criterion, scheduler, model\n",
        "):\n",
        "    net_raw, net_motion, net_conv_2, net_ff = networks\n",
        "    net_raw.train()\n",
        "    net_motion.train()\n",
        "    net_conv_2.train()\n",
        "    net_ff.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    for i, (raw, motion, labels) in enumerate(train_loader):\n",
        "        raw, motion, labels = raw.to(device), motion.to(\n",
        "            device), labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = get_model_output(\n",
        "            models=(net_raw, net_motion, net_conv_2, net_ff),\n",
        "            inputs=(raw, motion),\n",
        "            model_name=model\n",
        "        )\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    training_acc = 100 * correct / total\n",
        "    acc = getAccuarcy(val_loader, (net_raw, net_motion, net_conv_2, net_ff), model)\n",
        "    scheduler.step(acc)\n",
        "    return running_loss, training_acc, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "        POSE_MODEL, COORDINATE_DIM, JOINT_NUMBER, ORIGIN_JOINTS = (\n",
        "            (\"hrnet\", 2, 17, [5, 6, 11, 12])\n",
        "            if config.skeleton_type == \"2D\"\n",
        "            else (\"3danno\", 3, 25, [1])\n",
        "        )\n",
        "        DATASET_PATH = DATASET_NAME + \"_\" + POSE_MODEL + \".pkl\"\n",
        "\n",
        "        train_loader, val_loader = build_dataset(\n",
        "            DATASET_PATH,\n",
        "            config.out_frame_num,\n",
        "            COORDINATE_DIM,\n",
        "            JOINT_NUMBER,\n",
        "            config.sampling,\n",
        "            ORIGIN_JOINTS,\n",
        "            config.batch_size,\n",
        "        )\n",
        "        if config.model == \"2S-CNN\":\n",
        "            input_channels, input_features = 256, 512\n",
        "        elif config.model == \"1S-CNN\":\n",
        "            input_channels, input_features = 128, 1024\n",
        "        elif config.model == \"J-CNN\":\n",
        "            input_channels, input_features = 128, 512\n",
        "\n",
        "        net_raw, net_motion, net_conv_2, net_ff = build_network(\n",
        "            COORDINATE_DIM,\n",
        "            CLASS_NUMBER,\n",
        "            input_features=input_features,\n",
        "            input_channels=input_channels,\n",
        "            dropout_conv=config.dropout_conv,\n",
        "            dropout_ff=config.dropout_ff,\n",
        "        )\n",
        "        criterion, optimizer, scheduler = build_crit_optim_sched(\n",
        "            (net_raw, net_motion, net_conv_2, net_ff), config.learning_rate\n",
        "        )\n",
        "        net_dict = {\n",
        "            \"net_raw\": net_raw,\n",
        "            \"net_motion\": net_motion,\n",
        "            \"net_conv_2\": net_conv_2,\n",
        "            \"net_ff\": net_ff,\n",
        "        }\n",
        "\n",
        "        best_acc = 0.0\n",
        "        for epoch in tqdm(range(config.epochs)):\n",
        "            training_loss, training_acc, val_acc = train_epoch(\n",
        "                (net_raw, net_motion, net_conv_2, net_ff),\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                optimizer,\n",
        "                criterion,\n",
        "                scheduler,\n",
        "                model=config.model,\n",
        "            )\n",
        "            if val_acc > best_acc:\n",
        "                deleteModels(best_acc, net_dict, config.skeleton_type, FOLDER_STR)\n",
        "                best_acc = val_acc\n",
        "                saveModels(best_acc, net_dict, config.skeleton_type, FOLDER_STR)\n",
        "            print(\"[%d/%d] loss: %.1f\" % (epoch + 1, config.epochs, training_loss))\n",
        "            print(\"Training acc: %.3f %%\" % training_acc)\n",
        "            print(\"Validation acc: %.3f %%\" % val_acc)\n",
        "            metrics = {\n",
        "                \"train/train_loss\": training_loss,\n",
        "                \"train/epoch\": epoch + 1,\n",
        "                \"train/train_accuracy\": training_acc,\n",
        "            }\n",
        "            val_metrics = {\n",
        "                \"val/val_accuracy\": val_acc,\n",
        "                \"val/val_max_accuracy\": best_acc,\n",
        "            }\n",
        "            wandb.log({**metrics, **val_metrics})\n",
        "        print(\"Finished Training\")\n",
        "        print(f\"Best acc: {best_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mqjwh8yg with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: ntu60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_conv: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_ff: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 120\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_Type: xsub\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: J-CNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_frame_num: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling: even_spaced\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: ReduceLROnPlateau\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tskeleton_type: 2D\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\jakub.nitkiewicz\\OneDrive - Wabtec Corporation\\Desktop\\Notebooks\\wandb\\run-20240116_213757-mqjwh8yg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/mqjwh8yg' target=\"_blank\">playful-sweep-1</a></strong> to <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/mqjwh8yg' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/mqjwh8yg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a588df735ffd44049a26a939b7eb0451",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/56578 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 1,819,964\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43a15197eab44a99bd29174add04fd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/120] loss: 794.4\n",
            "Training acc: 28.790 %\n",
            "Validation acc: 38.982 %\n",
            "[2/120] loss: 415.9\n",
            "Training acc: 57.207 %\n",
            "Validation acc: 61.412 %\n",
            "[3/120] loss: 323.7\n",
            "Training acc: 65.915 %\n",
            "Validation acc: 69.030 %\n",
            "[4/120] loss: 276.3\n",
            "Training acc: 70.659 %\n",
            "Validation acc: 69.503 %\n",
            "[5/120] loss: 245.2\n",
            "Training acc: 73.917 %\n",
            "Validation acc: 70.589 %\n",
            "[6/120] loss: 222.9\n",
            "Training acc: 76.142 %\n",
            "Validation acc: 72.512 %\n",
            "[7/120] loss: 206.4\n",
            "Training acc: 77.711 %\n",
            "Validation acc: 75.781 %\n",
            "[8/120] loss: 191.2\n",
            "Training acc: 79.340 %\n",
            "Validation acc: 75.065 %\n",
            "[9/120] loss: 177.9\n",
            "Training acc: 80.741 %\n",
            "Validation acc: 73.652 %\n",
            "[10/120] loss: 167.1\n",
            "Training acc: 81.687 %\n",
            "Validation acc: 76.794 %\n",
            "[11/120] loss: 156.9\n",
            "Training acc: 82.754 %\n",
            "Validation acc: 77.637 %\n",
            "[12/120] loss: 147.0\n",
            "Training acc: 83.685 %\n",
            "Validation acc: 77.776 %\n",
            "[13/120] loss: 139.9\n",
            "Training acc: 84.540 %\n",
            "Validation acc: 77.413 %\n",
            "[14/120] loss: 131.3\n",
            "Training acc: 85.241 %\n",
            "Validation acc: 75.914 %\n",
            "[15/120] loss: 126.8\n",
            "Training acc: 85.643 %\n",
            "Validation acc: 78.947 %\n",
            "[16/120] loss: 120.9\n",
            "Training acc: 86.403 %\n",
            "Validation acc: 80.136 %\n",
            "[17/120] loss: 117.2\n",
            "Training acc: 86.862 %\n",
            "Validation acc: 78.553 %\n",
            "[18/120] loss: 111.6\n",
            "Training acc: 87.531 %\n",
            "Validation acc: 78.644 %\n",
            "[19/120] loss: 107.2\n",
            "Training acc: 87.995 %\n",
            "Validation acc: 79.044 %\n",
            "[20/120] loss: 102.1\n",
            "Training acc: 88.524 %\n",
            "Validation acc: 78.341 %\n",
            "[21/120] loss: 99.9\n",
            "Training acc: 88.638 %\n",
            "Validation acc: 79.050 %\n",
            "[22/120] loss: 93.9\n",
            "Training acc: 89.357 %\n",
            "Validation acc: 80.227 %\n",
            "[23/120] loss: 91.5\n",
            "Training acc: 89.531 %\n",
            "Validation acc: 78.383 %\n",
            "[24/120] loss: 88.3\n",
            "Training acc: 90.095 %\n",
            "Validation acc: 79.608 %\n",
            "[25/120] loss: 84.5\n",
            "Training acc: 90.332 %\n",
            "Validation acc: 80.099 %\n",
            "[26/120] loss: 81.2\n",
            "Training acc: 90.744 %\n",
            "Validation acc: 80.567 %\n",
            "[27/120] loss: 79.2\n",
            "Training acc: 91.232 %\n",
            "Validation acc: 79.839 %\n",
            "[28/120] loss: 75.6\n",
            "Training acc: 91.260 %\n",
            "Validation acc: 80.451 %\n",
            "[29/120] loss: 72.6\n",
            "Training acc: 91.739 %\n",
            "Validation acc: 79.560 %\n",
            "[30/120] loss: 70.7\n",
            "Training acc: 92.018 %\n",
            "Validation acc: 79.766 %\n",
            "[31/120] loss: 69.5\n",
            "Training acc: 92.095 %\n",
            "Validation acc: 79.778 %\n",
            "[32/120] loss: 67.4\n",
            "Training acc: 92.337 %\n",
            "Validation acc: 80.639 %\n",
            "[33/120] loss: 66.7\n",
            "Training acc: 92.422 %\n",
            "Validation acc: 80.785 %\n",
            "[34/120] loss: 61.7\n",
            "Training acc: 93.036 %\n",
            "Validation acc: 80.997 %\n",
            "[35/120] loss: 61.9\n",
            "Training acc: 92.821 %\n",
            "Validation acc: 80.403 %\n",
            "[36/120] loss: 61.6\n",
            "Training acc: 93.026 %\n",
            "Validation acc: 81.549 %\n",
            "[37/120] loss: 56.2\n",
            "Training acc: 93.592 %\n",
            "Validation acc: 80.203 %\n",
            "[38/120] loss: 59.0\n",
            "Training acc: 93.295 %\n",
            "Validation acc: 80.742 %\n",
            "[39/120] loss: 57.9\n",
            "Training acc: 93.452 %\n",
            "Validation acc: 79.632 %\n",
            "[40/120] loss: 55.4\n",
            "Training acc: 93.727 %\n",
            "Validation acc: 79.141 %\n",
            "[41/120] loss: 52.7\n",
            "Training acc: 93.951 %\n",
            "Validation acc: 80.706 %\n",
            "[42/120] loss: 53.5\n",
            "Training acc: 93.819 %\n",
            "Validation acc: 79.760 %\n",
            "[43/120] loss: 52.1\n",
            "Training acc: 94.076 %\n",
            "Validation acc: 78.656 %\n",
            "[44/120] loss: 51.3\n",
            "Training acc: 94.173 %\n",
            "Validation acc: 79.359 %\n",
            "[45/120] loss: 50.1\n",
            "Training acc: 94.440 %\n",
            "Validation acc: 80.421 %\n",
            "[46/120] loss: 46.8\n",
            "Training acc: 94.712 %\n",
            "Validation acc: 81.270 %\n",
            "Epoch 00047: reducing learning rate of group 0 to 5.0000e-04.\n",
            "[47/120] loss: 47.8\n",
            "Training acc: 94.615 %\n",
            "Validation acc: 79.008 %\n",
            "[48/120] loss: 35.3\n",
            "Training acc: 96.017 %\n",
            "Validation acc: 81.458 %\n",
            "[49/120] loss: 31.1\n",
            "Training acc: 96.560 %\n",
            "Validation acc: 79.924 %\n",
            "[50/120] loss: 29.3\n",
            "Training acc: 96.715 %\n",
            "Validation acc: 81.476 %\n",
            "[51/120] loss: 29.2\n",
            "Training acc: 96.710 %\n",
            "Validation acc: 80.779 %\n",
            "[52/120] loss: 27.9\n",
            "Training acc: 96.800 %\n",
            "Validation acc: 82.313 %\n",
            "[53/120] loss: 27.2\n",
            "Training acc: 96.924 %\n",
            "Validation acc: 81.246 %\n",
            "[54/120] loss: 27.5\n",
            "Training acc: 96.817 %\n",
            "Validation acc: 81.282 %\n",
            "[55/120] loss: 27.9\n",
            "Training acc: 96.929 %\n",
            "Validation acc: 80.494 %\n",
            "[56/120] loss: 28.3\n",
            "Training acc: 96.840 %\n",
            "Validation acc: 81.446 %\n",
            "[57/120] loss: 25.7\n",
            "Training acc: 97.166 %\n",
            "Validation acc: 81.428 %\n",
            "[58/120] loss: 25.5\n",
            "Training acc: 97.117 %\n",
            "Validation acc: 80.378 %\n",
            "[59/120] loss: 24.2\n",
            "Training acc: 97.211 %\n",
            "Validation acc: 79.487 %\n",
            "[60/120] loss: 24.5\n",
            "Training acc: 97.229 %\n",
            "Validation acc: 81.009 %\n",
            "[61/120] loss: 24.4\n",
            "Training acc: 97.286 %\n",
            "Validation acc: 81.713 %\n",
            "[62/120] loss: 26.2\n",
            "Training acc: 97.039 %\n",
            "Validation acc: 81.640 %\n",
            "Epoch 00063: reducing learning rate of group 0 to 2.5000e-04.\n",
            "[63/120] loss: 22.9\n",
            "Training acc: 97.329 %\n",
            "Validation acc: 82.295 %\n",
            "[64/120] loss: 19.1\n",
            "Training acc: 97.760 %\n",
            "Validation acc: 81.828 %\n",
            "[65/120] loss: 18.5\n",
            "Training acc: 97.935 %\n",
            "Validation acc: 81.652 %\n",
            "[66/120] loss: 17.1\n",
            "Training acc: 98.074 %\n",
            "Validation acc: 82.671 %\n",
            "[67/120] loss: 17.2\n",
            "Training acc: 98.129 %\n",
            "Validation acc: 82.204 %\n",
            "[68/120] loss: 16.5\n",
            "Training acc: 98.154 %\n",
            "Validation acc: 81.804 %\n",
            "[69/120] loss: 16.1\n",
            "Training acc: 98.259 %\n",
            "Validation acc: 82.974 %\n",
            "[70/120] loss: 15.7\n",
            "Training acc: 98.239 %\n",
            "Validation acc: 81.689 %\n",
            "[71/120] loss: 16.1\n",
            "Training acc: 98.256 %\n",
            "Validation acc: 82.635 %\n",
            "[72/120] loss: 16.0\n",
            "Training acc: 98.154 %\n",
            "Validation acc: 81.513 %\n",
            "[73/120] loss: 14.9\n",
            "Training acc: 98.359 %\n",
            "Validation acc: 82.750 %\n",
            "[74/120] loss: 14.4\n",
            "Training acc: 98.394 %\n",
            "Validation acc: 81.676 %\n",
            "[75/120] loss: 14.8\n",
            "Training acc: 98.331 %\n",
            "Validation acc: 81.871 %\n",
            "[76/120] loss: 13.9\n",
            "Training acc: 98.396 %\n",
            "Validation acc: 81.695 %\n",
            "[77/120] loss: 14.6\n",
            "Training acc: 98.401 %\n",
            "Validation acc: 82.313 %\n",
            "[78/120] loss: 14.0\n",
            "Training acc: 98.478 %\n",
            "Validation acc: 81.191 %\n",
            "[79/120] loss: 14.1\n",
            "Training acc: 98.379 %\n",
            "Validation acc: 81.889 %\n",
            "Epoch 00080: reducing learning rate of group 0 to 1.2500e-04.\n",
            "[80/120] loss: 13.8\n",
            "Training acc: 98.439 %\n",
            "Validation acc: 81.901 %\n",
            "[81/120] loss: 12.7\n",
            "Training acc: 98.571 %\n",
            "Validation acc: 81.331 %\n",
            "[82/120] loss: 11.9\n",
            "Training acc: 98.698 %\n",
            "Validation acc: 82.150 %\n",
            "[83/120] loss: 10.8\n",
            "Training acc: 98.823 %\n",
            "Validation acc: 82.507 %\n",
            "[84/120] loss: 10.9\n",
            "Training acc: 98.748 %\n",
            "Validation acc: 81.634 %\n",
            "[85/120] loss: 10.9\n",
            "Training acc: 98.820 %\n",
            "Validation acc: 81.391 %\n",
            "[86/120] loss: 10.6\n",
            "Training acc: 98.828 %\n",
            "Validation acc: 81.780 %\n",
            "[87/120] loss: 10.9\n",
            "Training acc: 98.795 %\n",
            "Validation acc: 82.507 %\n",
            "[88/120] loss: 10.8\n",
            "Training acc: 98.760 %\n",
            "Validation acc: 82.180 %\n",
            "[89/120] loss: 11.0\n",
            "Training acc: 98.803 %\n",
            "Validation acc: 81.598 %\n",
            "[90/120] loss: 10.5\n",
            "Training acc: 98.765 %\n",
            "Validation acc: 82.653 %\n",
            "Epoch 00091: reducing learning rate of group 0 to 6.2500e-05.\n",
            "[91/120] loss: 10.5\n",
            "Training acc: 98.810 %\n",
            "Validation acc: 82.174 %\n",
            "[92/120] loss: 9.9\n",
            "Training acc: 98.910 %\n",
            "Validation acc: 82.119 %\n",
            "[93/120] loss: 9.6\n",
            "Training acc: 98.997 %\n",
            "Validation acc: 82.611 %\n",
            "[94/120] loss: 9.0\n",
            "Training acc: 99.035 %\n",
            "Validation acc: 82.338 %\n",
            "[95/120] loss: 9.6\n",
            "Training acc: 99.002 %\n",
            "Validation acc: 81.695 %\n",
            "[96/120] loss: 9.3\n",
            "Training acc: 98.942 %\n",
            "Validation acc: 82.441 %\n",
            "[97/120] loss: 8.8\n",
            "Training acc: 99.035 %\n",
            "Validation acc: 82.477 %\n",
            "[98/120] loss: 9.1\n",
            "Training acc: 98.992 %\n",
            "Validation acc: 82.046 %\n",
            "[99/120] loss: 8.5\n",
            "Training acc: 99.090 %\n",
            "Validation acc: 82.756 %\n",
            "[100/120] loss: 8.8\n",
            "Training acc: 99.047 %\n",
            "Validation acc: 82.653 %\n",
            "[101/120] loss: 8.0\n",
            "Training acc: 99.122 %\n",
            "Validation acc: 82.732 %\n",
            "Epoch 00102: reducing learning rate of group 0 to 3.1250e-05.\n",
            "[102/120] loss: 8.2\n",
            "Training acc: 99.132 %\n",
            "Validation acc: 81.476 %\n",
            "[103/120] loss: 8.4\n",
            "Training acc: 99.092 %\n",
            "Validation acc: 81.658 %\n",
            "[104/120] loss: 7.7\n",
            "Training acc: 99.199 %\n",
            "Validation acc: 82.137 %\n",
            "[105/120] loss: 8.2\n",
            "Training acc: 99.087 %\n",
            "Validation acc: 81.701 %\n",
            "[106/120] loss: 7.9\n",
            "Training acc: 99.105 %\n",
            "Validation acc: 82.586 %\n",
            "[107/120] loss: 7.8\n",
            "Training acc: 99.142 %\n",
            "Validation acc: 82.168 %\n",
            "[108/120] loss: 8.0\n",
            "Training acc: 99.132 %\n",
            "Validation acc: 82.144 %\n",
            "[109/120] loss: 7.6\n",
            "Training acc: 99.154 %\n",
            "Validation acc: 81.713 %\n",
            "[110/120] loss: 7.5\n",
            "Training acc: 99.127 %\n",
            "Validation acc: 81.209 %\n",
            "[111/120] loss: 8.1\n",
            "Training acc: 99.127 %\n",
            "Validation acc: 82.277 %\n",
            "[112/120] loss: 7.6\n",
            "Training acc: 99.174 %\n",
            "Validation acc: 82.101 %\n",
            "Epoch 00113: reducing learning rate of group 0 to 1.5625e-05.\n",
            "[113/120] loss: 8.3\n",
            "Training acc: 99.090 %\n",
            "Validation acc: 82.513 %\n",
            "[114/120] loss: 7.8\n",
            "Training acc: 99.194 %\n",
            "Validation acc: 82.210 %\n",
            "[115/120] loss: 7.7\n",
            "Training acc: 99.154 %\n",
            "Validation acc: 82.671 %\n",
            "[116/120] loss: 7.4\n",
            "Training acc: 99.204 %\n",
            "Validation acc: 82.920 %\n",
            "[117/120] loss: 7.0\n",
            "Training acc: 99.274 %\n",
            "Validation acc: 82.786 %\n",
            "[118/120] loss: 7.1\n",
            "Training acc: 99.237 %\n",
            "Validation acc: 82.040 %\n",
            "[119/120] loss: 7.4\n",
            "Training acc: 99.197 %\n",
            "Validation acc: 82.338 %\n",
            "[120/120] loss: 7.8\n",
            "Training acc: 99.129 %\n",
            "Validation acc: 82.332 %\n",
            "Finished Training\n",
            "Best acc: 82.974\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d641b6b925047069f27f90a71bffab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_max_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/train_accuracy</td><td>99.12948</td></tr><tr><td>train/train_loss</td><td>7.82884</td></tr><tr><td>val/val_accuracy</td><td>82.33153</td></tr><tr><td>val/val_max_accuracy</td><td>82.97446</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">playful-sweep-1</strong> at: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/mqjwh8yg' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/mqjwh8yg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240116_213757-mqjwh8yg\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g2gsmkaj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: ntu60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_conv: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_ff: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 120\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_Type: xsub\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: J-CNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_frame_num: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling: even_spaced\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: ReduceLROnPlateau\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tskeleton_type: 2D\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\jakub.nitkiewicz\\OneDrive - Wabtec Corporation\\Desktop\\Notebooks\\wandb\\run-20240116_220345-g2gsmkaj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/g2gsmkaj' target=\"_blank\">sweet-sweep-2</a></strong> to <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/g2gsmkaj' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/g2gsmkaj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc6bd29edb8d49e39fa9f6e02191df61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/56578 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 1,819,964\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb74fd8ae9944caa99757b76899c3adc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/120] loss: 879.4\n",
            "Training acc: 24.048 %\n",
            "Validation acc: 42.415 %\n",
            "[2/120] loss: 465.5\n",
            "Training acc: 54.094 %\n",
            "Validation acc: 62.073 %\n",
            "[3/120] loss: 350.6\n",
            "Training acc: 64.049 %\n",
            "Validation acc: 70.037 %\n",
            "[4/120] loss: 294.5\n",
            "Training acc: 69.330 %\n",
            "Validation acc: 71.717 %\n",
            "[5/120] loss: 258.4\n",
            "Training acc: 72.914 %\n",
            "Validation acc: 72.845 %\n",
            "[6/120] loss: 237.1\n",
            "Training acc: 74.698 %\n",
            "Validation acc: 74.386 %\n",
            "[7/120] loss: 216.8\n",
            "Training acc: 76.967 %\n",
            "Validation acc: 76.460 %\n",
            "[8/120] loss: 201.0\n",
            "Training acc: 78.654 %\n",
            "Validation acc: 77.043 %\n",
            "[9/120] loss: 187.6\n",
            "Training acc: 79.542 %\n",
            "Validation acc: 75.126 %\n",
            "[10/120] loss: 174.1\n",
            "Training acc: 81.270 %\n",
            "Validation acc: 78.577 %\n",
            "[11/120] loss: 167.5\n",
            "Training acc: 81.754 %\n",
            "Validation acc: 79.220 %\n",
            "[12/120] loss: 157.0\n",
            "Training acc: 82.896 %\n",
            "Validation acc: 78.880 %\n",
            "[13/120] loss: 147.4\n",
            "Training acc: 83.764 %\n",
            "Validation acc: 79.087 %\n",
            "[14/120] loss: 140.9\n",
            "Training acc: 84.503 %\n",
            "Validation acc: 79.736 %\n",
            "[15/120] loss: 134.8\n",
            "Training acc: 85.411 %\n",
            "Validation acc: 80.093 %\n",
            "[16/120] loss: 127.0\n",
            "Training acc: 86.054 %\n",
            "Validation acc: 80.172 %\n",
            "[17/120] loss: 121.4\n",
            "Training acc: 86.668 %\n",
            "Validation acc: 78.935 %\n",
            "[18/120] loss: 116.5\n",
            "Training acc: 87.266 %\n",
            "Validation acc: 80.742 %\n",
            "[19/120] loss: 112.4\n",
            "Training acc: 87.434 %\n",
            "Validation acc: 81.549 %\n",
            "[20/120] loss: 108.6\n",
            "Training acc: 88.067 %\n",
            "Validation acc: 80.688 %\n",
            "[21/120] loss: 104.1\n",
            "Training acc: 88.434 %\n",
            "Validation acc: 82.022 %\n",
            "[22/120] loss: 96.9\n",
            "Training acc: 89.055 %\n",
            "Validation acc: 81.034 %\n",
            "[23/120] loss: 95.5\n",
            "Training acc: 89.454 %\n",
            "Validation acc: 80.591 %\n",
            "[24/120] loss: 92.1\n",
            "Training acc: 89.656 %\n",
            "Validation acc: 81.858 %\n",
            "[25/120] loss: 86.7\n",
            "Training acc: 90.235 %\n",
            "Validation acc: 81.895 %\n",
            "[26/120] loss: 83.2\n",
            "Training acc: 90.634 %\n",
            "Validation acc: 81.088 %\n",
            "[27/120] loss: 84.8\n",
            "Training acc: 90.509 %\n",
            "Validation acc: 81.949 %\n",
            "[28/120] loss: 81.3\n",
            "Training acc: 90.769 %\n",
            "Validation acc: 81.901 %\n",
            "[29/120] loss: 77.7\n",
            "Training acc: 91.295 %\n",
            "Validation acc: 80.797 %\n",
            "[30/120] loss: 73.8\n",
            "Training acc: 91.699 %\n",
            "Validation acc: 81.076 %\n",
            "[31/120] loss: 72.3\n",
            "Training acc: 91.836 %\n",
            "Validation acc: 82.295 %\n",
            "[32/120] loss: 71.6\n",
            "Training acc: 92.133 %\n",
            "Validation acc: 81.021 %\n",
            "[33/120] loss: 69.4\n",
            "Training acc: 92.061 %\n",
            "Validation acc: 80.912 %\n",
            "[34/120] loss: 66.3\n",
            "Training acc: 92.687 %\n",
            "Validation acc: 82.453 %\n",
            "[35/120] loss: 64.7\n",
            "Training acc: 92.549 %\n",
            "Validation acc: 81.598 %\n",
            "[36/120] loss: 61.8\n",
            "Training acc: 93.018 %\n",
            "Validation acc: 82.210 %\n",
            "[37/120] loss: 59.8\n",
            "Training acc: 93.233 %\n",
            "Validation acc: 81.343 %\n",
            "[38/120] loss: 62.7\n",
            "Training acc: 92.717 %\n",
            "Validation acc: 82.253 %\n",
            "[39/120] loss: 58.6\n",
            "Training acc: 93.325 %\n",
            "Validation acc: 81.883 %\n",
            "[40/120] loss: 56.5\n",
            "Training acc: 93.522 %\n",
            "Validation acc: 82.920 %\n",
            "[41/120] loss: 55.0\n",
            "Training acc: 93.714 %\n",
            "Validation acc: 82.253 %\n",
            "[42/120] loss: 53.8\n",
            "Training acc: 94.083 %\n",
            "Validation acc: 81.731 %\n",
            "[43/120] loss: 52.0\n",
            "Training acc: 94.044 %\n",
            "Validation acc: 80.530 %\n",
            "[44/120] loss: 52.6\n",
            "Training acc: 94.011 %\n",
            "Validation acc: 82.774 %\n",
            "[45/120] loss: 51.2\n",
            "Training acc: 94.206 %\n",
            "Validation acc: 81.658 %\n",
            "[46/120] loss: 50.5\n",
            "Training acc: 94.288 %\n",
            "Validation acc: 82.702 %\n",
            "[47/120] loss: 48.0\n",
            "Training acc: 94.652 %\n",
            "Validation acc: 82.568 %\n",
            "[48/120] loss: 45.6\n",
            "Training acc: 94.789 %\n",
            "Validation acc: 82.053 %\n",
            "[49/120] loss: 48.7\n",
            "Training acc: 94.478 %\n",
            "Validation acc: 82.429 %\n",
            "[50/120] loss: 46.8\n",
            "Training acc: 94.627 %\n",
            "Validation acc: 82.562 %\n",
            "Epoch 00051: reducing learning rate of group 0 to 2.5000e-04.\n",
            "[51/120] loss: 45.1\n",
            "Training acc: 94.759 %\n",
            "Validation acc: 80.870 %\n",
            "[52/120] loss: 34.9\n",
            "Training acc: 96.081 %\n",
            "Validation acc: 82.447 %\n",
            "[53/120] loss: 30.3\n",
            "Training acc: 96.588 %\n",
            "Validation acc: 82.702 %\n",
            "[54/120] loss: 29.6\n",
            "Training acc: 96.707 %\n",
            "Validation acc: 81.986 %\n",
            "[55/120] loss: 28.6\n",
            "Training acc: 96.802 %\n",
            "Validation acc: 82.623 %\n",
            "[56/120] loss: 28.9\n",
            "Training acc: 96.745 %\n",
            "Validation acc: 82.823 %\n",
            "[57/120] loss: 28.4\n",
            "Training acc: 96.797 %\n",
            "Validation acc: 83.078 %\n",
            "[58/120] loss: 27.4\n",
            "Training acc: 96.915 %\n",
            "Validation acc: 82.750 %\n",
            "[59/120] loss: 25.8\n",
            "Training acc: 97.189 %\n",
            "Validation acc: 82.702 %\n",
            "[60/120] loss: 26.1\n",
            "Training acc: 97.002 %\n",
            "Validation acc: 82.920 %\n",
            "[61/120] loss: 25.1\n",
            "Training acc: 97.196 %\n",
            "Validation acc: 83.545 %\n",
            "[62/120] loss: 25.0\n",
            "Training acc: 97.279 %\n",
            "Validation acc: 82.174 %\n",
            "[63/120] loss: 23.7\n",
            "Training acc: 97.301 %\n",
            "Validation acc: 82.865 %\n",
            "[64/120] loss: 24.5\n",
            "Training acc: 97.239 %\n",
            "Validation acc: 82.477 %\n",
            "[65/120] loss: 24.8\n",
            "Training acc: 97.211 %\n",
            "Validation acc: 82.313 %\n",
            "[66/120] loss: 25.1\n",
            "Training acc: 97.219 %\n",
            "Validation acc: 82.556 %\n",
            "[67/120] loss: 23.1\n",
            "Training acc: 97.351 %\n",
            "Validation acc: 82.768 %\n",
            "[68/120] loss: 23.3\n",
            "Training acc: 97.349 %\n",
            "Validation acc: 82.617 %\n",
            "[69/120] loss: 23.8\n",
            "Training acc: 97.381 %\n",
            "Validation acc: 82.871 %\n",
            "[70/120] loss: 22.7\n",
            "Training acc: 97.436 %\n",
            "Validation acc: 82.489 %\n",
            "[71/120] loss: 22.1\n",
            "Training acc: 97.546 %\n",
            "Validation acc: 83.284 %\n",
            "Epoch 00072: reducing learning rate of group 0 to 1.2500e-04.\n",
            "[72/120] loss: 21.7\n",
            "Training acc: 97.551 %\n",
            "Validation acc: 83.150 %\n",
            "[73/120] loss: 18.1\n",
            "Training acc: 97.965 %\n",
            "Validation acc: 83.272 %\n",
            "[74/120] loss: 16.7\n",
            "Training acc: 98.164 %\n",
            "Validation acc: 83.126 %\n",
            "[75/120] loss: 14.8\n",
            "Training acc: 98.346 %\n",
            "Validation acc: 83.023 %\n",
            "[76/120] loss: 15.2\n",
            "Training acc: 98.336 %\n",
            "Validation acc: 82.805 %\n",
            "[77/120] loss: 14.8\n",
            "Training acc: 98.376 %\n",
            "Validation acc: 83.108 %\n",
            "[78/120] loss: 14.9\n",
            "Training acc: 98.334 %\n",
            "Validation acc: 83.490 %\n",
            "[79/120] loss: 15.3\n",
            "Training acc: 98.264 %\n",
            "Validation acc: 82.932 %\n",
            "[80/120] loss: 14.5\n",
            "Training acc: 98.409 %\n",
            "Validation acc: 82.877 %\n",
            "[81/120] loss: 14.0\n",
            "Training acc: 98.493 %\n",
            "Validation acc: 82.811 %\n",
            "[82/120] loss: 13.4\n",
            "Training acc: 98.513 %\n",
            "Validation acc: 82.665 %\n",
            "Epoch 00083: reducing learning rate of group 0 to 6.2500e-05.\n",
            "[83/120] loss: 14.0\n",
            "Training acc: 98.471 %\n",
            "Validation acc: 83.411 %\n",
            "[84/120] loss: 12.7\n",
            "Training acc: 98.638 %\n",
            "Validation acc: 82.871 %\n",
            "[85/120] loss: 11.9\n",
            "Training acc: 98.713 %\n",
            "Validation acc: 83.502 %\n",
            "[86/120] loss: 11.5\n",
            "Training acc: 98.720 %\n",
            "Validation acc: 83.363 %\n",
            "[87/120] loss: 11.4\n",
            "Training acc: 98.738 %\n",
            "Validation acc: 83.642 %\n",
            "[88/120] loss: 11.5\n",
            "Training acc: 98.773 %\n",
            "Validation acc: 83.047 %\n",
            "[89/120] loss: 10.8\n",
            "Training acc: 98.860 %\n",
            "Validation acc: 83.326 %\n",
            "[90/120] loss: 11.2\n",
            "Training acc: 98.725 %\n",
            "Validation acc: 83.854 %\n",
            "[91/120] loss: 10.3\n",
            "Training acc: 98.925 %\n",
            "Validation acc: 83.344 %\n",
            "[92/120] loss: 10.5\n",
            "Training acc: 98.850 %\n",
            "Validation acc: 82.938 %\n",
            "[93/120] loss: 11.7\n",
            "Training acc: 98.718 %\n",
            "Validation acc: 83.205 %\n",
            "[94/120] loss: 10.6\n",
            "Training acc: 98.770 %\n",
            "Validation acc: 83.084 %\n",
            "[95/120] loss: 10.2\n",
            "Training acc: 98.932 %\n",
            "Validation acc: 82.635 %\n",
            "[96/120] loss: 10.6\n",
            "Training acc: 98.868 %\n",
            "Validation acc: 83.278 %\n",
            "[97/120] loss: 10.2\n",
            "Training acc: 98.922 %\n",
            "Validation acc: 83.557 %\n",
            "[98/120] loss: 10.6\n",
            "Training acc: 98.823 %\n",
            "Validation acc: 83.144 %\n",
            "[99/120] loss: 9.9\n",
            "Training acc: 98.957 %\n",
            "Validation acc: 83.581 %\n",
            "[100/120] loss: 9.5\n",
            "Training acc: 99.002 %\n",
            "Validation acc: 83.587 %\n",
            "Epoch 00101: reducing learning rate of group 0 to 3.1250e-05.\n",
            "[101/120] loss: 9.2\n",
            "Training acc: 99.030 %\n",
            "Validation acc: 83.514 %\n",
            "[102/120] loss: 9.3\n",
            "Training acc: 98.980 %\n",
            "Validation acc: 83.599 %\n",
            "[103/120] loss: 9.1\n",
            "Training acc: 99.052 %\n",
            "Validation acc: 83.435 %\n",
            "[104/120] loss: 9.0\n",
            "Training acc: 99.035 %\n",
            "Validation acc: 83.454 %\n",
            "[105/120] loss: 8.4\n",
            "Training acc: 99.080 %\n",
            "Validation acc: 82.920 %\n",
            "[106/120] loss: 8.3\n",
            "Training acc: 99.132 %\n",
            "Validation acc: 83.787 %\n",
            "[107/120] loss: 8.8\n",
            "Training acc: 99.022 %\n",
            "Validation acc: 82.993 %\n",
            "[108/120] loss: 9.2\n",
            "Training acc: 99.055 %\n",
            "Validation acc: 83.338 %\n",
            "[109/120] loss: 8.3\n",
            "Training acc: 99.055 %\n",
            "Validation acc: 83.344 %\n",
            "[110/120] loss: 8.7\n",
            "Training acc: 99.077 %\n",
            "Validation acc: 82.956 %\n",
            "[111/120] loss: 8.7\n",
            "Training acc: 99.032 %\n",
            "Validation acc: 83.205 %\n",
            "Epoch 00112: reducing learning rate of group 0 to 1.5625e-05.\n",
            "[112/120] loss: 8.4\n",
            "Training acc: 99.117 %\n",
            "Validation acc: 83.387 %\n",
            "[113/120] loss: 8.0\n",
            "Training acc: 99.107 %\n",
            "Validation acc: 83.866 %\n",
            "[114/120] loss: 8.3\n",
            "Training acc: 99.120 %\n",
            "Validation acc: 83.539 %\n",
            "[115/120] loss: 8.4\n",
            "Training acc: 99.132 %\n",
            "Validation acc: 83.144 %\n",
            "[116/120] loss: 7.3\n",
            "Training acc: 99.254 %\n",
            "Validation acc: 83.951 %\n",
            "[117/120] loss: 7.4\n",
            "Training acc: 99.222 %\n",
            "Validation acc: 83.818 %\n",
            "[118/120] loss: 7.6\n",
            "Training acc: 99.184 %\n",
            "Validation acc: 83.454 %\n",
            "[119/120] loss: 8.0\n",
            "Training acc: 99.187 %\n",
            "Validation acc: 83.272 %\n",
            "[120/120] loss: 7.5\n",
            "Training acc: 99.179 %\n",
            "Validation acc: 83.630 %\n",
            "Finished Training\n",
            "Best acc: 83.951\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f08270621110452fbf3251ccf282144c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_max_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/train_accuracy</td><td>99.17937</td></tr><tr><td>train/train_loss</td><td>7.46443</td></tr><tr><td>val/val_accuracy</td><td>83.62953</td></tr><tr><td>val/val_max_accuracy</td><td>83.95099</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sweet-sweep-2</strong> at: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/g2gsmkaj' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/g2gsmkaj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240116_220345-g2gsmkaj\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: si095jo7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: ntu60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_conv: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_ff: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 120\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_Type: xsub\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: J-CNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_frame_num: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling: even_spaced\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: ReduceLROnPlateau\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tskeleton_type: 2D\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\jakub.nitkiewicz\\OneDrive - Wabtec Corporation\\Desktop\\Notebooks\\wandb\\run-20240116_222840-si095jo7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/si095jo7' target=\"_blank\">stellar-sweep-3</a></strong> to <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/si095jo7' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/si095jo7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50f5c1d32401497a922e5227b7fc0164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/56578 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 1,819,964\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c768cb6963f406d96aac31e3790807d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/120] loss: 1008.1\n",
            "Training acc: 16.246 %\n",
            "Validation acc: 34.009 %\n",
            "[2/120] loss: 602.8\n",
            "Training acc: 44.194 %\n",
            "Validation acc: 55.965 %\n",
            "[3/120] loss: 433.8\n",
            "Training acc: 57.786 %\n",
            "Validation acc: 66.210 %\n",
            "[4/120] loss: 349.1\n",
            "Training acc: 65.109 %\n",
            "Validation acc: 68.654 %\n",
            "[5/120] loss: 299.7\n",
            "Training acc: 69.597 %\n",
            "Validation acc: 71.007 %\n",
            "[6/120] loss: 267.1\n",
            "Training acc: 72.562 %\n",
            "Validation acc: 74.077 %\n",
            "[7/120] loss: 243.0\n",
            "Training acc: 74.742 %\n",
            "Validation acc: 76.157 %\n",
            "[8/120] loss: 223.1\n",
            "Training acc: 76.733 %\n",
            "Validation acc: 75.999 %\n",
            "[9/120] loss: 205.6\n",
            "Training acc: 78.484 %\n",
            "Validation acc: 76.030 %\n",
            "[10/120] loss: 192.2\n",
            "Training acc: 79.769 %\n",
            "Validation acc: 78.826 %\n",
            "[11/120] loss: 181.3\n",
            "Training acc: 80.629 %\n",
            "Validation acc: 78.674 %\n",
            "[12/120] loss: 171.0\n",
            "Training acc: 81.662 %\n",
            "Validation acc: 78.874 %\n",
            "[13/120] loss: 161.5\n",
            "Training acc: 82.547 %\n",
            "Validation acc: 79.942 %\n",
            "[14/120] loss: 152.3\n",
            "Training acc: 83.453 %\n",
            "Validation acc: 79.869 %\n",
            "[15/120] loss: 143.9\n",
            "Training acc: 84.423 %\n",
            "Validation acc: 80.294 %\n",
            "[16/120] loss: 139.2\n",
            "Training acc: 84.977 %\n",
            "Validation acc: 80.742 %\n",
            "[17/120] loss: 131.6\n",
            "Training acc: 85.688 %\n",
            "Validation acc: 80.815 %\n",
            "[18/120] loss: 128.8\n",
            "Training acc: 86.019 %\n",
            "Validation acc: 81.294 %\n",
            "[19/120] loss: 122.0\n",
            "Training acc: 86.603 %\n",
            "Validation acc: 81.640 %\n",
            "[20/120] loss: 118.4\n",
            "Training acc: 87.057 %\n",
            "Validation acc: 81.149 %\n",
            "[21/120] loss: 113.5\n",
            "Training acc: 87.488 %\n",
            "Validation acc: 81.962 %\n",
            "[22/120] loss: 106.7\n",
            "Training acc: 88.202 %\n",
            "Validation acc: 82.077 %\n",
            "[23/120] loss: 103.2\n",
            "Training acc: 88.651 %\n",
            "Validation acc: 81.774 %\n",
            "[24/120] loss: 100.5\n",
            "Training acc: 88.583 %\n",
            "Validation acc: 83.247 %\n",
            "[25/120] loss: 96.2\n",
            "Training acc: 89.544 %\n",
            "Validation acc: 81.962 %\n",
            "[26/120] loss: 92.3\n",
            "Training acc: 89.688 %\n",
            "Validation acc: 81.822 %\n",
            "[27/120] loss: 90.8\n",
            "Training acc: 89.968 %\n",
            "Validation acc: 82.920 %\n",
            "[28/120] loss: 87.7\n",
            "Training acc: 90.352 %\n",
            "Validation acc: 81.761 %\n",
            "[29/120] loss: 83.9\n",
            "Training acc: 90.629 %\n",
            "Validation acc: 82.295 %\n",
            "[30/120] loss: 81.8\n",
            "Training acc: 90.893 %\n",
            "Validation acc: 82.228 %\n",
            "[31/120] loss: 80.0\n",
            "Training acc: 91.178 %\n",
            "Validation acc: 82.453 %\n",
            "[32/120] loss: 77.5\n",
            "Training acc: 91.425 %\n",
            "Validation acc: 82.702 %\n",
            "[33/120] loss: 74.2\n",
            "Training acc: 91.749 %\n",
            "Validation acc: 82.641 %\n",
            "[34/120] loss: 72.6\n",
            "Training acc: 91.898 %\n",
            "Validation acc: 83.199 %\n",
            "Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.\n",
            "[35/120] loss: 71.0\n",
            "Training acc: 92.021 %\n",
            "Validation acc: 82.883 %\n",
            "[36/120] loss: 56.8\n",
            "Training acc: 93.709 %\n",
            "Validation acc: 84.139 %\n",
            "[37/120] loss: 52.6\n",
            "Training acc: 94.171 %\n",
            "Validation acc: 84.103 %\n",
            "[38/120] loss: 50.7\n",
            "Training acc: 94.298 %\n",
            "Validation acc: 83.757 %\n",
            "[39/120] loss: 51.2\n",
            "Training acc: 94.278 %\n",
            "Validation acc: 83.769 %\n",
            "[40/120] loss: 48.6\n",
            "Training acc: 94.565 %\n",
            "Validation acc: 84.248 %\n",
            "[41/120] loss: 47.1\n",
            "Training acc: 94.720 %\n",
            "Validation acc: 83.854 %\n",
            "[42/120] loss: 46.2\n",
            "Training acc: 94.809 %\n",
            "Validation acc: 83.854 %\n",
            "[43/120] loss: 45.2\n",
            "Training acc: 94.804 %\n",
            "Validation acc: 83.933 %\n",
            "[44/120] loss: 43.6\n",
            "Training acc: 95.159 %\n",
            "Validation acc: 84.157 %\n",
            "[45/120] loss: 41.9\n",
            "Training acc: 95.348 %\n",
            "Validation acc: 84.297 %\n",
            "[46/120] loss: 43.4\n",
            "Training acc: 95.181 %\n",
            "Validation acc: 84.084 %\n",
            "[47/120] loss: 41.6\n",
            "Training acc: 95.373 %\n",
            "Validation acc: 84.606 %\n",
            "[48/120] loss: 40.8\n",
            "Training acc: 95.463 %\n",
            "Validation acc: 84.181 %\n",
            "[49/120] loss: 40.5\n",
            "Training acc: 95.505 %\n",
            "Validation acc: 84.133 %\n",
            "[50/120] loss: 40.4\n",
            "Training acc: 95.415 %\n",
            "Validation acc: 83.702 %\n",
            "[51/120] loss: 38.9\n",
            "Training acc: 95.642 %\n",
            "Validation acc: 83.842 %\n",
            "[52/120] loss: 38.3\n",
            "Training acc: 95.824 %\n",
            "Validation acc: 83.860 %\n",
            "[53/120] loss: 37.3\n",
            "Training acc: 95.914 %\n",
            "Validation acc: 83.945 %\n",
            "[54/120] loss: 37.9\n",
            "Training acc: 95.707 %\n",
            "Validation acc: 84.103 %\n",
            "[55/120] loss: 36.6\n",
            "Training acc: 95.922 %\n",
            "Validation acc: 83.648 %\n",
            "[56/120] loss: 37.4\n",
            "Training acc: 95.832 %\n",
            "Validation acc: 83.878 %\n",
            "[57/120] loss: 34.9\n",
            "Training acc: 96.054 %\n",
            "Validation acc: 84.333 %\n",
            "Epoch 00058: reducing learning rate of group 0 to 6.2500e-05.\n",
            "[58/120] loss: 34.8\n",
            "Training acc: 96.104 %\n",
            "Validation acc: 83.939 %\n",
            "[59/120] loss: 31.2\n",
            "Training acc: 96.675 %\n",
            "Validation acc: 84.327 %\n",
            "[60/120] loss: 28.8\n",
            "Training acc: 96.787 %\n",
            "Validation acc: 84.533 %\n",
            "[61/120] loss: 27.6\n",
            "Training acc: 97.002 %\n",
            "Validation acc: 84.460 %\n",
            "[62/120] loss: 27.3\n",
            "Training acc: 96.959 %\n",
            "Validation acc: 84.661 %\n",
            "[63/120] loss: 26.2\n",
            "Training acc: 97.044 %\n",
            "Validation acc: 84.667 %\n",
            "[64/120] loss: 25.6\n",
            "Training acc: 97.194 %\n",
            "Validation acc: 84.752 %\n",
            "[65/120] loss: 25.9\n",
            "Training acc: 97.127 %\n",
            "Validation acc: 84.770 %\n",
            "[66/120] loss: 24.9\n",
            "Training acc: 97.304 %\n",
            "Validation acc: 84.539 %\n",
            "[67/120] loss: 24.5\n",
            "Training acc: 97.346 %\n",
            "Validation acc: 84.903 %\n",
            "[68/120] loss: 24.8\n",
            "Training acc: 97.239 %\n",
            "Validation acc: 84.746 %\n",
            "[69/120] loss: 24.2\n",
            "Training acc: 97.366 %\n",
            "Validation acc: 84.964 %\n",
            "[70/120] loss: 23.9\n",
            "Training acc: 97.436 %\n",
            "Validation acc: 84.309 %\n",
            "[71/120] loss: 23.6\n",
            "Training acc: 97.433 %\n",
            "Validation acc: 84.570 %\n",
            "[72/120] loss: 23.1\n",
            "Training acc: 97.401 %\n",
            "Validation acc: 84.460 %\n",
            "[73/120] loss: 23.0\n",
            "Training acc: 97.418 %\n",
            "Validation acc: 84.800 %\n",
            "[74/120] loss: 22.4\n",
            "Training acc: 97.513 %\n",
            "Validation acc: 84.794 %\n",
            "[75/120] loss: 22.6\n",
            "Training acc: 97.528 %\n",
            "Validation acc: 84.509 %\n",
            "[76/120] loss: 22.8\n",
            "Training acc: 97.526 %\n",
            "Validation acc: 84.830 %\n",
            "[77/120] loss: 22.3\n",
            "Training acc: 97.523 %\n",
            "Validation acc: 84.764 %\n",
            "[78/120] loss: 21.3\n",
            "Training acc: 97.571 %\n",
            "Validation acc: 84.649 %\n",
            "[79/120] loss: 21.9\n",
            "Training acc: 97.605 %\n",
            "Validation acc: 84.649 %\n",
            "Epoch 00080: reducing learning rate of group 0 to 3.1250e-05.\n",
            "[80/120] loss: 21.7\n",
            "Training acc: 97.483 %\n",
            "Validation acc: 84.764 %\n",
            "[81/120] loss: 19.0\n",
            "Training acc: 97.900 %\n",
            "Validation acc: 84.837 %\n",
            "[82/120] loss: 18.8\n",
            "Training acc: 98.002 %\n",
            "Validation acc: 85.079 %\n",
            "[83/120] loss: 18.3\n",
            "Training acc: 98.020 %\n",
            "Validation acc: 84.934 %\n",
            "[84/120] loss: 17.4\n",
            "Training acc: 98.104 %\n",
            "Validation acc: 84.915 %\n",
            "[85/120] loss: 17.9\n",
            "Training acc: 98.022 %\n",
            "Validation acc: 84.800 %\n",
            "[86/120] loss: 17.7\n",
            "Training acc: 98.097 %\n",
            "Validation acc: 85.031 %\n",
            "[87/120] loss: 18.0\n",
            "Training acc: 98.022 %\n",
            "Validation acc: 85.103 %\n",
            "[88/120] loss: 17.3\n",
            "Training acc: 98.134 %\n",
            "Validation acc: 85.188 %\n",
            "[89/120] loss: 17.1\n",
            "Training acc: 98.189 %\n",
            "Validation acc: 84.885 %\n",
            "[90/120] loss: 17.8\n",
            "Training acc: 98.042 %\n",
            "Validation acc: 84.982 %\n",
            "[91/120] loss: 17.3\n",
            "Training acc: 98.159 %\n",
            "Validation acc: 84.685 %\n",
            "[92/120] loss: 16.2\n",
            "Training acc: 98.269 %\n",
            "Validation acc: 84.915 %\n",
            "[93/120] loss: 17.1\n",
            "Training acc: 98.219 %\n",
            "Validation acc: 84.782 %\n",
            "[94/120] loss: 16.0\n",
            "Training acc: 98.296 %\n",
            "Validation acc: 85.031 %\n",
            "[95/120] loss: 16.8\n",
            "Training acc: 98.122 %\n",
            "Validation acc: 85.025 %\n",
            "[96/120] loss: 16.3\n",
            "Training acc: 98.237 %\n",
            "Validation acc: 84.885 %\n",
            "[97/120] loss: 16.3\n",
            "Training acc: 98.207 %\n",
            "Validation acc: 84.928 %\n",
            "[98/120] loss: 15.7\n",
            "Training acc: 98.279 %\n",
            "Validation acc: 84.921 %\n",
            "[99/120] loss: 15.5\n",
            "Training acc: 98.364 %\n",
            "Validation acc: 85.376 %\n",
            "[100/120] loss: 16.2\n",
            "Training acc: 98.254 %\n",
            "Validation acc: 85.358 %\n",
            "[101/120] loss: 15.8\n",
            "Training acc: 98.269 %\n",
            "Validation acc: 85.273 %\n",
            "[102/120] loss: 15.2\n",
            "Training acc: 98.371 %\n",
            "Validation acc: 84.873 %\n",
            "[103/120] loss: 15.5\n",
            "Training acc: 98.344 %\n",
            "Validation acc: 84.606 %\n",
            "[104/120] loss: 14.9\n",
            "Training acc: 98.426 %\n",
            "Validation acc: 84.818 %\n",
            "[105/120] loss: 15.4\n",
            "Training acc: 98.379 %\n",
            "Validation acc: 84.370 %\n",
            "[106/120] loss: 15.3\n",
            "Training acc: 98.416 %\n",
            "Validation acc: 84.909 %\n",
            "[107/120] loss: 14.7\n",
            "Training acc: 98.456 %\n",
            "Validation acc: 84.964 %\n",
            "[108/120] loss: 14.7\n",
            "Training acc: 98.426 %\n",
            "Validation acc: 84.903 %\n",
            "[109/120] loss: 14.6\n",
            "Training acc: 98.451 %\n",
            "Validation acc: 84.849 %\n",
            "Epoch 00110: reducing learning rate of group 0 to 1.5625e-05.\n",
            "[110/120] loss: 14.0\n",
            "Training acc: 98.498 %\n",
            "Validation acc: 84.685 %\n",
            "[111/120] loss: 13.8\n",
            "Training acc: 98.513 %\n",
            "Validation acc: 84.855 %\n",
            "[112/120] loss: 13.3\n",
            "Training acc: 98.558 %\n",
            "Validation acc: 84.897 %\n",
            "[113/120] loss: 14.3\n",
            "Training acc: 98.486 %\n",
            "Validation acc: 85.213 %\n",
            "[114/120] loss: 13.6\n",
            "Training acc: 98.459 %\n",
            "Validation acc: 85.128 %\n",
            "[115/120] loss: 13.2\n",
            "Training acc: 98.606 %\n",
            "Validation acc: 85.152 %\n",
            "[116/120] loss: 13.7\n",
            "Training acc: 98.571 %\n",
            "Validation acc: 85.461 %\n",
            "[117/120] loss: 13.0\n",
            "Training acc: 98.621 %\n",
            "Validation acc: 85.467 %\n",
            "[118/120] loss: 12.7\n",
            "Training acc: 98.631 %\n",
            "Validation acc: 85.146 %\n",
            "[119/120] loss: 13.5\n",
            "Training acc: 98.568 %\n",
            "Validation acc: 85.261 %\n",
            "[120/120] loss: 13.3\n",
            "Training acc: 98.518 %\n",
            "Validation acc: 85.031 %\n",
            "Finished Training\n",
            "Best acc: 85.467\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7684245472e4513ae43c5ce2c8bb3b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_max_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/train_accuracy</td><td>98.51837</td></tr><tr><td>train/train_loss</td><td>13.31981</td></tr><tr><td>val/val_accuracy</td><td>85.03063</td></tr><tr><td>val/val_max_accuracy</td><td>85.46734</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-sweep-3</strong> at: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/si095jo7' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/si095jo7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240116_222840-si095jo7\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 52mog7l6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: ntu60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_conv: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_ff: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 120\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_Type: xsub\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: J-CNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_frame_num: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling: even_spaced\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: ReduceLROnPlateau\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tskeleton_type: 2D\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\jakub.nitkiewicz\\OneDrive - Wabtec Corporation\\Desktop\\Notebooks\\wandb\\run-20240116_225323-52mog7l6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/52mog7l6' target=\"_blank\">earnest-sweep-4</a></strong> to <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/52mog7l6' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/52mog7l6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70ff393b3f7d4ea49018ce8baa401600",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/56578 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 1,819,964\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "547b212015274a49a1e0cb49086f323b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/120] loss: 1142.1\n",
            "Training acc: 8.653 %\n",
            "Validation acc: 18.032 %\n",
            "[2/120] loss: 906.9\n",
            "Training acc: 22.708 %\n",
            "Validation acc: 35.531 %\n",
            "[3/120] loss: 694.1\n",
            "Training acc: 38.063 %\n",
            "Validation acc: 52.241 %\n",
            "[4/120] loss: 559.0\n",
            "Training acc: 48.936 %\n",
            "Validation acc: 57.148 %\n",
            "[5/120] loss: 469.6\n",
            "Training acc: 56.072 %\n",
            "Validation acc: 61.582 %\n",
            "[6/120] loss: 404.7\n",
            "Training acc: 61.370 %\n",
            "Validation acc: 68.211 %\n",
            "[7/120] loss: 358.1\n",
            "Training acc: 65.421 %\n",
            "Validation acc: 69.085 %\n",
            "[8/120] loss: 324.9\n",
            "Training acc: 68.068 %\n",
            "Validation acc: 72.433 %\n",
            "[9/120] loss: 295.6\n",
            "Training acc: 70.597 %\n",
            "Validation acc: 71.881 %\n",
            "[10/120] loss: 273.6\n",
            "Training acc: 72.390 %\n",
            "Validation acc: 74.337 %\n",
            "[11/120] loss: 257.0\n",
            "Training acc: 74.084 %\n",
            "Validation acc: 75.344 %\n",
            "[12/120] loss: 240.1\n",
            "Training acc: 75.503 %\n",
            "Validation acc: 75.933 %\n",
            "[13/120] loss: 227.3\n",
            "Training acc: 76.691 %\n",
            "Validation acc: 76.727 %\n",
            "[14/120] loss: 215.2\n",
            "Training acc: 77.793 %\n",
            "Validation acc: 77.716 %\n",
            "[15/120] loss: 204.0\n",
            "Training acc: 79.020 %\n",
            "Validation acc: 77.692 %\n",
            "[16/120] loss: 195.2\n",
            "Training acc: 79.564 %\n",
            "Validation acc: 78.256 %\n",
            "[17/120] loss: 186.5\n",
            "Training acc: 80.544 %\n",
            "Validation acc: 79.335 %\n",
            "[18/120] loss: 179.7\n",
            "Training acc: 81.308 %\n",
            "Validation acc: 79.548 %\n",
            "[19/120] loss: 172.9\n",
            "Training acc: 81.727 %\n",
            "Validation acc: 79.535 %\n",
            "[20/120] loss: 165.6\n",
            "Training acc: 82.684 %\n",
            "Validation acc: 79.123 %\n",
            "[21/120] loss: 161.3\n",
            "Training acc: 82.876 %\n",
            "Validation acc: 79.808 %\n",
            "[22/120] loss: 154.0\n",
            "Training acc: 83.685 %\n",
            "Validation acc: 80.093 %\n",
            "[23/120] loss: 147.4\n",
            "Training acc: 84.291 %\n",
            "Validation acc: 80.069 %\n",
            "[24/120] loss: 144.2\n",
            "Training acc: 84.790 %\n",
            "Validation acc: 80.178 %\n",
            "[25/120] loss: 139.1\n",
            "Training acc: 85.134 %\n",
            "Validation acc: 80.591 %\n",
            "[26/120] loss: 135.4\n",
            "Training acc: 85.443 %\n",
            "Validation acc: 80.263 %\n",
            "[27/120] loss: 130.0\n",
            "Training acc: 85.964 %\n",
            "Validation acc: 81.288 %\n",
            "[28/120] loss: 125.6\n",
            "Training acc: 86.615 %\n",
            "Validation acc: 81.367 %\n",
            "[29/120] loss: 121.0\n",
            "Training acc: 87.087 %\n",
            "Validation acc: 81.634 %\n",
            "[30/120] loss: 118.5\n",
            "Training acc: 87.227 %\n",
            "Validation acc: 81.476 %\n",
            "[31/120] loss: 114.2\n",
            "Training acc: 87.818 %\n",
            "Validation acc: 81.173 %\n",
            "[32/120] loss: 111.8\n",
            "Training acc: 88.032 %\n",
            "Validation acc: 80.761 %\n",
            "[33/120] loss: 108.4\n",
            "Training acc: 88.401 %\n",
            "Validation acc: 81.616 %\n",
            "[34/120] loss: 106.3\n",
            "Training acc: 88.461 %\n",
            "Validation acc: 81.919 %\n",
            "[35/120] loss: 104.8\n",
            "Training acc: 88.583 %\n",
            "Validation acc: 81.046 %\n",
            "[36/120] loss: 100.7\n",
            "Training acc: 89.077 %\n",
            "Validation acc: 81.828 %\n",
            "[37/120] loss: 96.3\n",
            "Training acc: 89.367 %\n",
            "Validation acc: 82.999 %\n",
            "[38/120] loss: 93.1\n",
            "Training acc: 89.671 %\n",
            "Validation acc: 81.385 %\n",
            "[39/120] loss: 94.6\n",
            "Training acc: 89.676 %\n",
            "Validation acc: 81.743 %\n",
            "[40/120] loss: 90.5\n",
            "Training acc: 90.142 %\n",
            "Validation acc: 81.161 %\n",
            "[41/120] loss: 89.3\n",
            "Training acc: 90.170 %\n",
            "Validation acc: 82.053 %\n",
            "[42/120] loss: 85.7\n",
            "Training acc: 90.654 %\n",
            "Validation acc: 81.549 %\n",
            "[43/120] loss: 83.6\n",
            "Training acc: 90.754 %\n",
            "Validation acc: 81.707 %\n",
            "[44/120] loss: 79.8\n",
            "Training acc: 91.195 %\n",
            "Validation acc: 82.416 %\n",
            "[45/120] loss: 79.1\n",
            "Training acc: 91.295 %\n",
            "Validation acc: 82.580 %\n",
            "[46/120] loss: 79.4\n",
            "Training acc: 91.170 %\n",
            "Validation acc: 82.598 %\n",
            "[47/120] loss: 76.2\n",
            "Training acc: 91.589 %\n",
            "Validation acc: 83.023 %\n",
            "[48/120] loss: 74.2\n",
            "Training acc: 91.819 %\n",
            "Validation acc: 83.156 %\n",
            "[49/120] loss: 72.6\n",
            "Training acc: 91.983 %\n",
            "Validation acc: 83.205 %\n",
            "[50/120] loss: 71.4\n",
            "Training acc: 92.238 %\n",
            "Validation acc: 82.526 %\n",
            "[51/120] loss: 70.5\n",
            "Training acc: 92.215 %\n",
            "Validation acc: 82.107 %\n",
            "[52/120] loss: 70.0\n",
            "Training acc: 92.248 %\n",
            "Validation acc: 82.338 %\n",
            "[53/120] loss: 65.6\n",
            "Training acc: 92.689 %\n",
            "Validation acc: 82.665 %\n",
            "[54/120] loss: 66.0\n",
            "Training acc: 92.831 %\n",
            "Validation acc: 82.641 %\n",
            "[55/120] loss: 65.6\n",
            "Training acc: 92.896 %\n",
            "Validation acc: 82.398 %\n",
            "[56/120] loss: 65.5\n",
            "Training acc: 92.764 %\n",
            "Validation acc: 82.799 %\n",
            "[57/120] loss: 61.5\n",
            "Training acc: 93.278 %\n",
            "Validation acc: 83.387 %\n",
            "[58/120] loss: 60.1\n",
            "Training acc: 93.328 %\n",
            "Validation acc: 82.234 %\n",
            "[59/120] loss: 59.9\n",
            "Training acc: 93.353 %\n",
            "Validation acc: 83.508 %\n",
            "[60/120] loss: 56.5\n",
            "Training acc: 93.824 %\n",
            "Validation acc: 82.847 %\n",
            "[61/120] loss: 58.0\n",
            "Training acc: 93.527 %\n",
            "Validation acc: 83.162 %\n",
            "[62/120] loss: 56.2\n",
            "Training acc: 93.694 %\n",
            "Validation acc: 82.829 %\n",
            "[63/120] loss: 54.7\n",
            "Training acc: 93.996 %\n",
            "Validation acc: 82.732 %\n",
            "[64/120] loss: 55.6\n",
            "Training acc: 93.929 %\n",
            "Validation acc: 82.871 %\n",
            "[65/120] loss: 54.1\n",
            "Training acc: 94.148 %\n",
            "Validation acc: 83.369 %\n",
            "[66/120] loss: 52.7\n",
            "Training acc: 94.156 %\n",
            "Validation acc: 82.968 %\n",
            "[67/120] loss: 50.7\n",
            "Training acc: 94.390 %\n",
            "Validation acc: 83.278 %\n",
            "[68/120] loss: 50.2\n",
            "Training acc: 94.550 %\n",
            "Validation acc: 81.913 %\n",
            "[69/120] loss: 49.8\n",
            "Training acc: 94.433 %\n",
            "Validation acc: 83.084 %\n",
            "Epoch 00070: reducing learning rate of group 0 to 5.0000e-05.\n",
            "[70/120] loss: 50.0\n",
            "Training acc: 94.490 %\n",
            "Validation acc: 82.890 %\n",
            "[71/120] loss: 43.7\n",
            "Training acc: 95.183 %\n",
            "Validation acc: 83.278 %\n",
            "[72/120] loss: 39.9\n",
            "Training acc: 95.652 %\n",
            "Validation acc: 83.587 %\n",
            "[73/120] loss: 37.1\n",
            "Training acc: 96.017 %\n",
            "Validation acc: 83.520 %\n",
            "[74/120] loss: 37.0\n",
            "Training acc: 95.864 %\n",
            "Validation acc: 83.751 %\n",
            "[75/120] loss: 37.2\n",
            "Training acc: 95.892 %\n",
            "Validation acc: 84.054 %\n",
            "[76/120] loss: 35.5\n",
            "Training acc: 96.074 %\n",
            "Validation acc: 83.915 %\n",
            "[77/120] loss: 35.8\n",
            "Training acc: 96.104 %\n",
            "Validation acc: 83.909 %\n",
            "[78/120] loss: 34.7\n",
            "Training acc: 96.201 %\n",
            "Validation acc: 83.569 %\n",
            "[79/120] loss: 34.8\n",
            "Training acc: 96.226 %\n",
            "Validation acc: 84.066 %\n",
            "[80/120] loss: 34.7\n",
            "Training acc: 96.156 %\n",
            "Validation acc: 83.890 %\n",
            "[81/120] loss: 34.0\n",
            "Training acc: 96.311 %\n",
            "Validation acc: 83.884 %\n",
            "[82/120] loss: 33.2\n",
            "Training acc: 96.381 %\n",
            "Validation acc: 84.430 %\n",
            "[83/120] loss: 33.5\n",
            "Training acc: 96.303 %\n",
            "Validation acc: 83.727 %\n",
            "[84/120] loss: 32.8\n",
            "Training acc: 96.383 %\n",
            "Validation acc: 84.066 %\n",
            "[85/120] loss: 32.5\n",
            "Training acc: 96.570 %\n",
            "Validation acc: 83.981 %\n",
            "[86/120] loss: 31.3\n",
            "Training acc: 96.548 %\n",
            "Validation acc: 84.000 %\n",
            "[87/120] loss: 30.9\n",
            "Training acc: 96.683 %\n",
            "Validation acc: 84.054 %\n",
            "[88/120] loss: 29.9\n",
            "Training acc: 96.755 %\n",
            "Validation acc: 83.696 %\n",
            "[89/120] loss: 29.6\n",
            "Training acc: 96.857 %\n",
            "Validation acc: 84.297 %\n",
            "[90/120] loss: 31.5\n",
            "Training acc: 96.618 %\n",
            "Validation acc: 83.993 %\n",
            "[91/120] loss: 29.3\n",
            "Training acc: 96.822 %\n",
            "Validation acc: 83.781 %\n",
            "[92/120] loss: 29.1\n",
            "Training acc: 96.912 %\n",
            "Validation acc: 83.460 %\n",
            "Epoch 00093: reducing learning rate of group 0 to 2.5000e-05.\n",
            "[93/120] loss: 29.8\n",
            "Training acc: 96.752 %\n",
            "Validation acc: 84.151 %\n",
            "[94/120] loss: 26.1\n",
            "Training acc: 97.286 %\n",
            "Validation acc: 84.315 %\n",
            "[95/120] loss: 26.3\n",
            "Training acc: 97.134 %\n",
            "Validation acc: 84.060 %\n",
            "[96/120] loss: 24.7\n",
            "Training acc: 97.418 %\n",
            "Validation acc: 84.509 %\n",
            "[97/120] loss: 25.0\n",
            "Training acc: 97.319 %\n",
            "Validation acc: 84.151 %\n",
            "[98/120] loss: 24.3\n",
            "Training acc: 97.451 %\n",
            "Validation acc: 84.497 %\n",
            "[99/120] loss: 23.3\n",
            "Training acc: 97.571 %\n",
            "Validation acc: 84.849 %\n",
            "[100/120] loss: 23.7\n",
            "Training acc: 97.438 %\n",
            "Validation acc: 84.770 %\n",
            "[101/120] loss: 23.1\n",
            "Training acc: 97.590 %\n",
            "Validation acc: 84.594 %\n",
            "[102/120] loss: 23.7\n",
            "Training acc: 97.598 %\n",
            "Validation acc: 84.630 %\n",
            "[103/120] loss: 22.9\n",
            "Training acc: 97.556 %\n",
            "Validation acc: 84.509 %\n",
            "[104/120] loss: 23.3\n",
            "Training acc: 97.438 %\n",
            "Validation acc: 84.363 %\n",
            "[105/120] loss: 23.1\n",
            "Training acc: 97.418 %\n",
            "Validation acc: 84.133 %\n",
            "[106/120] loss: 22.8\n",
            "Training acc: 97.668 %\n",
            "Validation acc: 84.606 %\n",
            "[107/120] loss: 22.0\n",
            "Training acc: 97.720 %\n",
            "Validation acc: 84.351 %\n",
            "[108/120] loss: 22.4\n",
            "Training acc: 97.595 %\n",
            "Validation acc: 84.394 %\n",
            "[109/120] loss: 22.3\n",
            "Training acc: 97.600 %\n",
            "Validation acc: 84.424 %\n",
            "Epoch 00110: reducing learning rate of group 0 to 1.2500e-05.\n",
            "[110/120] loss: 21.1\n",
            "Training acc: 97.743 %\n",
            "Validation acc: 84.357 %\n",
            "[111/120] loss: 20.3\n",
            "Training acc: 97.815 %\n",
            "Validation acc: 84.570 %\n",
            "[112/120] loss: 20.5\n",
            "Training acc: 97.892 %\n",
            "Validation acc: 84.436 %\n",
            "[113/120] loss: 20.2\n",
            "Training acc: 97.905 %\n",
            "Validation acc: 84.915 %\n",
            "[114/120] loss: 19.5\n",
            "Training acc: 97.915 %\n",
            "Validation acc: 84.691 %\n",
            "[115/120] loss: 19.7\n",
            "Training acc: 97.900 %\n",
            "Validation acc: 84.721 %\n",
            "[116/120] loss: 19.4\n",
            "Training acc: 97.932 %\n",
            "Validation acc: 84.794 %\n",
            "[117/120] loss: 19.2\n",
            "Training acc: 97.945 %\n",
            "Validation acc: 84.606 %\n",
            "[118/120] loss: 19.2\n",
            "Training acc: 98.049 %\n",
            "Validation acc: 84.612 %\n",
            "[119/120] loss: 19.1\n",
            "Training acc: 98.044 %\n",
            "Validation acc: 84.436 %\n",
            "[120/120] loss: 18.6\n",
            "Training acc: 98.012 %\n",
            "Validation acc: 84.266 %\n",
            "Finished Training\n",
            "Best acc: 84.915\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e44951a0580849de9de5bbfa05613820",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_max_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/train_accuracy</td><td>98.01202</td></tr><tr><td>train/train_loss</td><td>18.59952</td></tr><tr><td>val/val_accuracy</td><td>84.26639</td></tr><tr><td>val/val_max_accuracy</td><td>84.91539</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earnest-sweep-4</strong> at: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/52mog7l6' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/52mog7l6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240116_225323-52mog7l6\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7tvqkasx with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: ntu60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_conv: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_ff: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 120\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_Type: xsub\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: J-CNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_frame_num: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling: even_spaced\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: ReduceLROnPlateau\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tskeleton_type: 2D\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\jakub.nitkiewicz\\OneDrive - Wabtec Corporation\\Desktop\\Notebooks\\wandb\\run-20240116_231822-7tvqkasx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/7tvqkasx' target=\"_blank\">graceful-sweep-5</a></strong> to <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/sweeps/cvou4ffa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/7tvqkasx' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/7tvqkasx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8de34ed02a7743fca4bafdb1f0b06d45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/56578 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 1,819,964\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f7ec3ac8fb145bdb4e715c8c47c4876",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/120] loss: 1192.7\n",
            "Training acc: 6.291 %\n",
            "Validation acc: 11.191 %\n",
            "[2/120] loss: 1071.1\n",
            "Training acc: 12.280 %\n",
            "Validation acc: 22.715 %\n",
            "[3/120] loss: 943.9\n",
            "Training acc: 20.369 %\n",
            "Validation acc: 30.703 %\n",
            "[4/120] loss: 816.0\n",
            "Training acc: 29.261 %\n",
            "Validation acc: 38.909 %\n",
            "[5/120] loss: 712.7\n",
            "Training acc: 36.764 %\n",
            "Validation acc: 46.443 %\n",
            "[6/120] loss: 625.8\n",
            "Training acc: 44.214 %\n",
            "Validation acc: 53.836 %\n",
            "[7/120] loss: 561.0\n",
            "Training acc: 49.153 %\n",
            "Validation acc: 57.227 %\n",
            "[8/120] loss: 509.0\n",
            "Training acc: 53.157 %\n",
            "Validation acc: 61.576 %\n",
            "[9/120] loss: 463.5\n",
            "Training acc: 56.906 %\n",
            "Validation acc: 62.025 %\n",
            "[10/120] loss: 425.7\n",
            "Training acc: 59.806 %\n",
            "Validation acc: 66.246 %\n",
            "[11/120] loss: 394.5\n",
            "Training acc: 62.680 %\n",
            "Validation acc: 66.834 %\n",
            "[12/120] loss: 367.9\n",
            "Training acc: 64.835 %\n",
            "Validation acc: 68.569 %\n",
            "[13/120] loss: 345.8\n",
            "Training acc: 66.357 %\n",
            "Validation acc: 69.540 %\n",
            "[14/120] loss: 327.3\n",
            "Training acc: 68.020 %\n",
            "Validation acc: 70.947 %\n",
            "[15/120] loss: 309.9\n",
            "Training acc: 69.746 %\n",
            "Validation acc: 70.698 %\n",
            "[16/120] loss: 294.7\n",
            "Training acc: 70.866 %\n",
            "Validation acc: 72.463 %\n",
            "[17/120] loss: 280.3\n",
            "Training acc: 72.288 %\n",
            "Validation acc: 73.876 %\n",
            "[18/120] loss: 269.6\n",
            "Training acc: 73.169 %\n",
            "Validation acc: 75.089 %\n",
            "[19/120] loss: 259.9\n",
            "Training acc: 73.914 %\n",
            "Validation acc: 75.569 %\n",
            "[20/120] loss: 249.7\n",
            "Training acc: 75.044 %\n",
            "Validation acc: 75.447 %\n",
            "[21/120] loss: 239.8\n",
            "Training acc: 75.720 %\n",
            "Validation acc: 76.127 %\n",
            "[22/120] loss: 231.4\n",
            "Training acc: 76.656 %\n",
            "Validation acc: 75.872 %\n",
            "[23/120] loss: 223.7\n",
            "Training acc: 77.304 %\n",
            "Validation acc: 76.873 %\n",
            "[24/120] loss: 217.8\n",
            "Training acc: 77.888 %\n",
            "Validation acc: 77.607 %\n",
            "[25/120] loss: 209.3\n",
            "Training acc: 78.728 %\n",
            "Validation acc: 77.946 %\n",
            "[26/120] loss: 204.7\n",
            "Training acc: 78.901 %\n",
            "Validation acc: 77.309 %\n",
            "[27/120] loss: 199.0\n",
            "Training acc: 79.671 %\n",
            "Validation acc: 77.431 %\n",
            "[28/120] loss: 192.4\n",
            "Training acc: 80.160 %\n",
            "Validation acc: 78.031 %\n",
            "[29/120] loss: 187.5\n",
            "Training acc: 80.499 %\n",
            "Validation acc: 78.201 %\n",
            "[30/120] loss: 182.2\n",
            "Training acc: 81.190 %\n",
            "Validation acc: 79.184 %\n",
            "[31/120] loss: 178.6\n",
            "Training acc: 81.372 %\n",
            "Validation acc: 78.595 %\n",
            "[32/120] loss: 173.2\n",
            "Training acc: 82.126 %\n",
            "Validation acc: 79.002 %\n",
            "[33/120] loss: 169.2\n",
            "Training acc: 82.283 %\n",
            "Validation acc: 79.596 %\n",
            "[34/120] loss: 164.5\n",
            "Training acc: 82.814 %\n",
            "Validation acc: 79.729 %\n",
            "[35/120] loss: 162.2\n",
            "Training acc: 83.059 %\n",
            "Validation acc: 79.754 %\n",
            "[36/120] loss: 158.3\n",
            "Training acc: 83.425 %\n",
            "Validation acc: 80.294 %\n",
            "[37/120] loss: 153.0\n",
            "Training acc: 83.894 %\n",
            "Validation acc: 80.136 %\n",
            "[38/120] loss: 150.0\n",
            "Training acc: 84.129 %\n",
            "Validation acc: 79.899 %\n",
            "[39/120] loss: 147.8\n",
            "Training acc: 84.600 %\n",
            "Validation acc: 79.675 %\n",
            "[40/120] loss: 143.0\n",
            "Training acc: 84.902 %\n",
            "Validation acc: 81.003 %\n",
            "[41/120] loss: 141.8\n",
            "Training acc: 85.029 %\n",
            "Validation acc: 80.876 %\n",
            "[42/120] loss: 137.9\n",
            "Training acc: 85.500 %\n",
            "Validation acc: 80.615 %\n",
            "[43/120] loss: 134.9\n",
            "Training acc: 85.683 %\n",
            "Validation acc: 80.542 %\n",
            "[44/120] loss: 133.5\n",
            "Training acc: 85.872 %\n",
            "Validation acc: 81.191 %\n",
            "[45/120] loss: 128.3\n",
            "Training acc: 86.336 %\n",
            "Validation acc: 81.349 %\n",
            "[46/120] loss: 126.5\n",
            "Training acc: 86.593 %\n",
            "Validation acc: 81.167 %\n",
            "[47/120] loss: 123.8\n",
            "Training acc: 86.902 %\n",
            "Validation acc: 81.604 %\n",
            "[48/120] loss: 121.7\n",
            "Training acc: 87.207 %\n",
            "Validation acc: 81.701 %\n",
            "[49/120] loss: 119.7\n",
            "Training acc: 87.189 %\n",
            "Validation acc: 81.561 %\n",
            "[50/120] loss: 116.5\n",
            "Training acc: 87.788 %\n",
            "Validation acc: 81.604 %\n",
            "[51/120] loss: 115.2\n",
            "Training acc: 87.758 %\n",
            "Validation acc: 81.222 %\n",
            "[52/120] loss: 112.5\n",
            "Training acc: 87.815 %\n",
            "Validation acc: 81.313 %\n",
            "[53/120] loss: 108.1\n",
            "Training acc: 88.549 %\n",
            "Validation acc: 81.543 %\n",
            "[54/120] loss: 108.3\n",
            "Training acc: 88.337 %\n",
            "Validation acc: 81.822 %\n",
            "[55/120] loss: 106.8\n",
            "Training acc: 88.531 %\n",
            "Validation acc: 81.749 %\n",
            "[56/120] loss: 104.5\n",
            "Training acc: 88.756 %\n",
            "Validation acc: 81.640 %\n",
            "[57/120] loss: 101.1\n",
            "Training acc: 89.107 %\n",
            "Validation acc: 82.137 %\n",
            "[58/120] loss: 100.0\n",
            "Training acc: 89.277 %\n",
            "Validation acc: 81.846 %\n",
            "[59/120] loss: 99.7\n",
            "Training acc: 89.117 %\n",
            "Validation acc: 81.804 %\n",
            "[60/120] loss: 97.1\n",
            "Training acc: 89.491 %\n",
            "Validation acc: 81.968 %\n",
            "[61/120] loss: 94.5\n",
            "Training acc: 89.893 %\n",
            "Validation acc: 82.356 %\n",
            "[62/120] loss: 94.7\n",
            "Training acc: 89.841 %\n",
            "Validation acc: 82.386 %\n",
            "[63/120] loss: 91.7\n",
            "Training acc: 90.212 %\n",
            "Validation acc: 82.829 %\n",
            "[64/120] loss: 91.1\n",
            "Training acc: 90.255 %\n",
            "Validation acc: 82.477 %\n",
            "[65/120] loss: 88.3\n",
            "Training acc: 90.489 %\n",
            "Validation acc: 82.574 %\n",
            "[66/120] loss: 85.9\n",
            "Training acc: 90.609 %\n",
            "Validation acc: 82.022 %\n",
            "[67/120] loss: 86.0\n",
            "Training acc: 90.676 %\n",
            "Validation acc: 82.362 %\n",
            "[68/120] loss: 83.9\n",
            "Training acc: 90.951 %\n",
            "Validation acc: 82.198 %\n",
            "[69/120] loss: 82.1\n",
            "Training acc: 91.203 %\n",
            "Validation acc: 82.793 %\n",
            "[70/120] loss: 82.4\n",
            "Training acc: 91.203 %\n",
            "Validation acc: 82.222 %\n",
            "[71/120] loss: 81.9\n",
            "Training acc: 91.150 %\n",
            "Validation acc: 83.247 %\n",
            "[72/120] loss: 79.6\n",
            "Training acc: 91.307 %\n",
            "Validation acc: 82.265 %\n",
            "[73/120] loss: 77.4\n",
            "Training acc: 91.552 %\n",
            "Validation acc: 83.029 %\n",
            "[74/120] loss: 76.6\n",
            "Training acc: 91.701 %\n",
            "Validation acc: 82.744 %\n",
            "[75/120] loss: 74.1\n",
            "Training acc: 91.861 %\n",
            "Validation acc: 83.041 %\n",
            "[76/120] loss: 74.2\n",
            "Training acc: 91.881 %\n",
            "Validation acc: 82.914 %\n",
            "[77/120] loss: 73.0\n",
            "Training acc: 91.961 %\n",
            "Validation acc: 83.448 %\n",
            "[78/120] loss: 71.6\n",
            "Training acc: 92.095 %\n",
            "Validation acc: 83.314 %\n",
            "[79/120] loss: 71.5\n",
            "Training acc: 92.066 %\n",
            "Validation acc: 83.296 %\n",
            "[80/120] loss: 69.7\n",
            "Training acc: 92.442 %\n",
            "Validation acc: 83.272 %\n",
            "[81/120] loss: 69.2\n",
            "Training acc: 92.335 %\n",
            "Validation acc: 83.132 %\n",
            "[82/120] loss: 68.0\n",
            "Training acc: 92.475 %\n",
            "Validation acc: 83.047 %\n",
            "[83/120] loss: 65.5\n",
            "Training acc: 92.988 %\n",
            "Validation acc: 83.078 %\n",
            "[84/120] loss: 66.4\n",
            "Training acc: 92.647 %\n",
            "Validation acc: 83.047 %\n",
            "[85/120] loss: 63.8\n",
            "Training acc: 93.103 %\n",
            "Validation acc: 83.630 %\n",
            "[86/120] loss: 65.4\n",
            "Training acc: 92.809 %\n",
            "Validation acc: 83.053 %\n",
            "[87/120] loss: 63.5\n",
            "Training acc: 93.118 %\n",
            "Validation acc: 83.551 %\n",
            "[88/120] loss: 63.3\n",
            "Training acc: 93.151 %\n",
            "Validation acc: 83.593 %\n",
            "[89/120] loss: 61.0\n",
            "Training acc: 93.452 %\n",
            "Validation acc: 83.041 %\n",
            "[90/120] loss: 60.9\n",
            "Training acc: 93.358 %\n",
            "Validation acc: 83.490 %\n",
            "[91/120] loss: 59.8\n",
            "Training acc: 93.612 %\n",
            "Validation acc: 83.096 %\n",
            "[92/120] loss: 57.9\n",
            "Training acc: 93.759 %\n",
            "Validation acc: 83.017 %\n",
            "[93/120] loss: 57.4\n",
            "Training acc: 93.717 %\n",
            "Validation acc: 83.569 %\n",
            "[94/120] loss: 57.2\n",
            "Training acc: 93.864 %\n",
            "Validation acc: 84.072 %\n",
            "[95/120] loss: 55.9\n",
            "Training acc: 93.884 %\n",
            "Validation acc: 83.181 %\n",
            "[96/120] loss: 55.8\n",
            "Training acc: 93.719 %\n",
            "Validation acc: 83.611 %\n",
            "[97/120] loss: 55.2\n",
            "Training acc: 93.919 %\n",
            "Validation acc: 83.472 %\n",
            "[98/120] loss: 54.5\n",
            "Training acc: 94.034 %\n",
            "Validation acc: 83.478 %\n",
            "[99/120] loss: 52.1\n",
            "Training acc: 94.290 %\n",
            "Validation acc: 83.557 %\n",
            "[100/120] loss: 54.2\n",
            "Training acc: 94.046 %\n",
            "Validation acc: 83.369 %\n",
            "[101/120] loss: 52.1\n",
            "Training acc: 94.233 %\n",
            "Validation acc: 83.611 %\n",
            "[102/120] loss: 51.7\n",
            "Training acc: 94.425 %\n",
            "Validation acc: 83.357 %\n",
            "[103/120] loss: 51.9\n",
            "Training acc: 94.295 %\n",
            "Validation acc: 83.623 %\n",
            "[104/120] loss: 51.0\n",
            "Training acc: 94.368 %\n",
            "Validation acc: 83.848 %\n",
            "Epoch 00105: reducing learning rate of group 0 to 2.5000e-05.\n",
            "[105/120] loss: 50.4\n",
            "Training acc: 94.510 %\n",
            "Validation acc: 83.399 %\n",
            "[106/120] loss: 44.7\n",
            "Training acc: 95.183 %\n",
            "Validation acc: 84.285 %\n",
            "[107/120] loss: 43.3\n",
            "Training acc: 95.428 %\n",
            "Validation acc: 84.018 %\n",
            "[108/120] loss: 42.5\n",
            "Training acc: 95.485 %\n",
            "Validation acc: 83.824 %\n",
            "[109/120] loss: 42.2\n",
            "Training acc: 95.430 %\n",
            "Validation acc: 83.708 %\n",
            "[110/120] loss: 40.9\n",
            "Training acc: 95.598 %\n",
            "Validation acc: 83.411 %\n",
            "[111/120] loss: 40.1\n",
            "Training acc: 95.642 %\n",
            "Validation acc: 83.617 %\n",
            "[112/120] loss: 39.8\n",
            "Training acc: 95.787 %\n",
            "Validation acc: 83.490 %\n",
            "[113/120] loss: 39.5\n",
            "Training acc: 95.680 %\n",
            "Validation acc: 83.933 %\n",
            "[114/120] loss: 39.2\n",
            "Training acc: 95.720 %\n",
            "Validation acc: 83.763 %\n",
            "[115/120] loss: 39.1\n",
            "Training acc: 95.837 %\n",
            "Validation acc: 84.084 %\n",
            "[116/120] loss: 38.3\n",
            "Training acc: 95.847 %\n",
            "Validation acc: 84.315 %\n",
            "[117/120] loss: 38.1\n",
            "Training acc: 95.849 %\n",
            "Validation acc: 84.242 %\n",
            "[118/120] loss: 38.5\n",
            "Training acc: 95.842 %\n",
            "Validation acc: 83.739 %\n",
            "[119/120] loss: 37.2\n",
            "Training acc: 96.009 %\n",
            "Validation acc: 84.036 %\n",
            "[120/120] loss: 36.7\n",
            "Training acc: 96.076 %\n",
            "Validation acc: 83.981 %\n",
            "Finished Training\n",
            "Best acc: 84.315\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aeef2e98729f43bab39ee90ec08c9b33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train/train_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val/val_max_accuracy</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/train_accuracy</td><td>96.07643</td></tr><tr><td>train/train_loss</td><td>36.68749</td></tr><tr><td>val/val_accuracy</td><td>83.98132</td></tr><tr><td>val/val_max_accuracy</td><td>84.31491</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">graceful-sweep-5</strong> at: <a href='https://wandb.ai/kubuseg/In%C5%BCynierka/runs/7tvqkasx' target=\"_blank\">https://wandb.ai/kubuseg/In%C5%BCynierka/runs/7tvqkasx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240116_231822-7tvqkasx\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ],
      "source": [
        "wandb.agent(sweep_id, train, count=12)\n",
        "# wandb.agent(sweep_id=\"noad1a5y\", project=\"InÅ¼ynierka\", function=train, count=12)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04bb8f5e1052450ca62598000f9b1fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1db910f4434878a7f599c6a8d2715e",
            "placeholder": "â",
            "style": "IPY_MODEL_0a7ffa51d210487e8603984dafe783ac",
            "value": " 56578/56578 [01:02&lt;00:00, 1035.49it/s]"
          }
        },
        "07802467a257410ab5b4a8b78dac12d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a7ffa51d210487e8603984dafe783ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e00b40790bf4da890fd033745db4907": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3a7ecaa93a495881063c9a85ba8f3d",
            "max": 56578,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e301fe1b1c8447728d3d82df902091ae",
            "value": 56578
          }
        },
        "5b1db910f4434878a7f599c6a8d2715e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a757d9be054a4a338f83c60a14c44d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb9ace46d7da4caf87a4321559a93091": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea48be358ca44582a42b2759874a689e",
              "IPY_MODEL_3e00b40790bf4da890fd033745db4907",
              "IPY_MODEL_04bb8f5e1052450ca62598000f9b1fef"
            ],
            "layout": "IPY_MODEL_d506a3cd6c724e8caf1df3844aa5c5fb"
          }
        },
        "cb3a7ecaa93a495881063c9a85ba8f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d506a3cd6c724e8caf1df3844aa5c5fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e301fe1b1c8447728d3d82df902091ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea48be358ca44582a42b2759874a689e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07802467a257410ab5b4a8b78dac12d3",
            "placeholder": "â",
            "style": "IPY_MODEL_a757d9be054a4a338f83c60a14c44d09",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
